{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [on]\n",
      "Loading model from: /home/maij/miniforge3/envs/sora/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install termcolor tqdm decord pytorchvideo\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict\n",
    "import logging\n",
    "import json\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from decord import VideoReader, cpu\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from pytorchvideo.transforms import ShortSideScale\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "\n",
    "from cal_flolpips import calculate_flolpips\n",
    "from cal_lpips import calculate_lpips\n",
    "from cal_psnr import calculate_psnr\n",
    "from cal_ssim import calculate_ssim\n",
    "\n",
    "# Suppress specific imageio FFmpeg warnings\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class to manage parameters for evaluation operations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        model_name: str = \"cogvideox\",\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        metrics: List[str] = [\"ssim\", \"psnr\", \"lpips\", \"flolpips\"],\n",
    "        batch_size: int = 2,\n",
    "        num_workers: int = 4,\n",
    "        num_frames: int = 100,\n",
    "        sample_rate: int = 1,\n",
    "        resolution: int = 128,\n",
    "        crop_size: int = None,\n",
    "        subset_size: int = None,\n",
    "        fvd_method: str = \"styleganv\",\n",
    "        output_json: str = \"result.json\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the configuration with default or specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - data_root (str): Root directory containing 'processed_gt_v2' and 'model_recon/<model_name>'.\n",
    "        - model_name (str): Name of the VAE model (e.g., 'cogvideox').\n",
    "        - device (str): Computation device ('cuda', 'cuda:0', 'cpu', etc.).\n",
    "        - dtype (str): Data type for computation ('float16' or 'bfloat16').\n",
    "        - metrics (List[str]): List of metrics to calculate.\n",
    "        - batch_size (int): Batch size for DataLoader.\n",
    "        - num_workers (int): Number of worker processes for DataLoader.\n",
    "        - num_frames (int): Number of frames to sample from each video.\n",
    "        - sample_rate (int): Sampling rate for frames.\n",
    "        - resolution (int): Short side size for scaling videos.\n",
    "        - crop_size (int): Size for center cropping. If None, no cropping is applied.\n",
    "        - subset_size (int): If specified, process only a subset of the dataset.\n",
    "        - fvd_method (str): Method for FVD calculation.\n",
    "        - output_json (str): Filename for saving the evaluation results.\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.metrics = metrics\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.num_frames = num_frames\n",
    "        self.sample_rate = sample_rate\n",
    "        self.resolution = resolution\n",
    "        self.crop_size = crop_size\n",
    "        self.subset_size = subset_size\n",
    "        self.fvd_method = fvd_method\n",
    "        self.output_json = output_json\n",
    "        \n",
    "        # Define paths\n",
    "        self.real_video_base = os.path.join(self.data_root, \"processed_gt_v2\")\n",
    "        self.generated_video_base = os.path.join(self.data_root, \"model_recon\", self.model_name)\n",
    "        self.result_json_path = os.path.join(self.generated_video_base, self.output_json)\n",
    "\n",
    "# VideoDataset Class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        real_video_dir: str,\n",
    "        generated_video_dir: str,\n",
    "        num_frames: int,\n",
    "        sample_rate: int = 1,\n",
    "        crop_size: int = None,\n",
    "        resolution: int = 128,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.real_video_files = self._get_sorted_videos(real_video_dir)\n",
    "        self.generated_video_files = self._get_sorted_videos(generated_video_dir)\n",
    "        self.num_frames = num_frames\n",
    "        self.sample_rate = sample_rate\n",
    "        self.crop_size = crop_size\n",
    "        self.short_size = resolution\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.real_video_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError\n",
    "        real_video_file = self.real_video_files[index]\n",
    "        generated_video_file = self.generated_video_files[index]\n",
    "        real_video_tensor = self._load_video(real_video_file)\n",
    "        generated_video_tensor = self._load_video(generated_video_file)\n",
    "        return {\"real\": real_video_tensor, \"generated\": generated_video_tensor}\n",
    "\n",
    "    def _load_video(self, video_path: str) -> torch.Tensor:\n",
    "        num_frames = self.num_frames\n",
    "        sample_rate = self.sample_rate\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))\n",
    "        total_frames = len(vr)\n",
    "        sample_frames_len = sample_rate * num_frames\n",
    "\n",
    "        if total_frames >= sample_frames_len:\n",
    "            s = 0\n",
    "            e = s + sample_frames_len\n",
    "            num_frames = num_frames\n",
    "        else:\n",
    "            s = 0\n",
    "            e = total_frames\n",
    "            num_frames = int(total_frames / sample_frames_len * num_frames)\n",
    "            print(\n",
    "                colored(f\"Sample_frames_len {sample_frames_len}, can only sample {num_frames * sample_rate} frames from {video_path}, total frames: {total_frames}\", \"yellow\")\n",
    "            )\n",
    "\n",
    "        frame_id_list = np.linspace(s, e - 1, num_frames, dtype=int)\n",
    "        video_data = vr.get_batch(frame_id_list).asnumpy()\n",
    "        video_data = torch.from_numpy(video_data)\n",
    "        video_data = video_data.permute(0, 3, 1, 2)  # (T, H, W, C) -> (C, T, H, W)\n",
    "        return self._preprocess(video_data)\n",
    "\n",
    "    def _preprocess(self, video_data: torch.Tensor) -> torch.Tensor:\n",
    "        transform = Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x / 255.0),\n",
    "                ShortSideScale(size=self.short_size),\n",
    "                CenterCropVideo(crop_size=self.crop_size),\n",
    "            ]\n",
    "        )\n",
    "        video_outputs = transform(video_data)\n",
    "        return video_outputs  # (C, T, H, W)\n",
    "\n",
    "    def _get_sorted_videos(self, folder_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves and sorts video files from a directory.\n",
    "        \n",
    "        Parameters:\n",
    "        - folder_path (str): Path to the video directory.\n",
    "        \n",
    "        Returns:\n",
    "        - List[str]: Sorted list of video file names.\n",
    "        \"\"\"\n",
    "        videos = [f for f in os.listdir(folder_path) \n",
    "                  if os.path.isfile(os.path.join(folder_path, f)) \n",
    "                  and os.path.splitext(f)[1].lower() in ['.mp4']]\n",
    "        return sorted(videos)\n",
    "\n",
    "# Custom Print Function with Colored Output\n",
    "def print_metric_result(dataset: str, metrics: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Prints the metric results for a single dataset with colored output.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset (str): Name of the dataset.\n",
    "    - metrics (Dict[str, float]): Dictionary of metric names and their corresponding scores.\n",
    "    \"\"\"\n",
    "    print(colored(f\"\\nDataset: {dataset}\", \"cyan\", attrs=[\"bold\"]))\n",
    "    for metric, score in metrics.items():\n",
    "        if metric.lower() in [\"ssim\", \"psnr\"]:\n",
    "            color = \"green\" if metric.lower() == \"ssim\" else \"yellow\"\n",
    "        elif metric.lower() in [\"lpips\", \"flolpips\"]:\n",
    "            color = \"magenta\"\n",
    "        else:\n",
    "            color = \"white\"\n",
    "        print(colored(f\"  {metric.upper()}: {score:.4f}\", color, attrs=[\"bold\"]))\n",
    "\n",
    "# calculate_common_metric Function\n",
    "def calculate_common_metric(\n",
    "    metrics: List[str],\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates the specified metrics for the given DataLoader.\n",
    "    \n",
    "    Parameters:\n",
    "    - metrics (List[str]): List of metrics to calculate.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset.\n",
    "    - device (torch.device): Device to perform calculations on.\n",
    "    \n",
    "    Returns:\n",
    "    - Dict[str, float]: Dictionary of metric names and their corresponding scores.\n",
    "    \"\"\"\n",
    "    metric_dict = {}\n",
    "    \n",
    "    print(colored(f\"Calculating Metrics: {', '.join(metrics)}\", \"blue\", attrs=[\"bold\"]))\n",
    "    \n",
    "    for metric in metrics:\n",
    "        score_list = []\n",
    "        for batch_data in tqdm(dataloader, desc=f\"Calculating {metric.upper()}\"):\n",
    "            real_videos = batch_data[\"real\"].to(device)\n",
    "            generated_videos = batch_data[\"generated\"].to(device)\n",
    "            assert real_videos.shape[2] == generated_videos.shape[2], \"Frame count mismatch between real and generated videos.\"\n",
    "            \n",
    "            if metric.lower() == \"ssim\":\n",
    "                tmp_list = list(calculate_ssim(real_videos, generated_videos)[\"value\"].values())\n",
    "            elif metric.lower() == \"psnr\":\n",
    "                tmp_list = list(calculate_psnr(real_videos, generated_videos)[\"value\"].values())\n",
    "            elif metric.lower() == \"flolpips\":\n",
    "                result = calculate_flolpips(real_videos, generated_videos, device)\n",
    "                tmp_list = list(result[\"value\"].values())\n",
    "            elif metric.lower() == \"lpips\":\n",
    "                tmp_list = list(calculate_lpips(real_videos, generated_videos, device)[\"value\"].values())\n",
    "            else:\n",
    "                print(colored(f\"Metric '{metric}' is not supported. Skipping.\", \"red\"))\n",
    "                continue\n",
    "            score_list += tmp_list\n",
    "        if score_list:\n",
    "            metric_dict[metric] = np.mean(score_list)\n",
    "    \n",
    "    return metric_dict\n",
    "\n",
    "# Evaluator Class\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluator class to handle the calculation of metrics for video datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.metrics = config.metrics\n",
    "        self.result = {}\n",
    "        self.avg_result = {}\n",
    "        \n",
    "    def get_datasets(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the list of dataset names from the real_video_base directory.\n",
    "        \n",
    "        Returns:\n",
    "        - List[str]: List of dataset directory names.\n",
    "        \"\"\"\n",
    "        datasets = [d for d in os.listdir(self.config.real_video_base) \n",
    "                   if os.path.isdir(os.path.join(self.config.real_video_base, d))]\n",
    "        return sorted(datasets)\n",
    "    \n",
    "    def calculate_metrics_for_dataset(self, dataset: str):\n",
    "        \"\"\"\n",
    "        Calculates metrics for a single dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataset (str): Name of the dataset.\n",
    "        \"\"\"\n",
    "        real_video_dir = os.path.join(self.config.real_video_base, dataset)\n",
    "        generated_video_dir = os.path.join(self.config.generated_video_base, dataset)\n",
    "        \n",
    "        # Check if generated_video_dir exists\n",
    "        if not os.path.exists(generated_video_dir):\n",
    "            print(colored(f\"Generated video directory does not exist for dataset: {dataset}. Skipping...\", \"red\"))\n",
    "            return\n",
    "        \n",
    "        # Initialize VideoDataset and DataLoader\n",
    "        dataset_obj = VideoDataset(\n",
    "            real_video_dir=real_video_dir,\n",
    "            generated_video_dir=generated_video_dir,\n",
    "            num_frames=self.config.num_frames,\n",
    "            sample_rate=self.config.sample_rate,\n",
    "            crop_size=self.config.crop_size,\n",
    "            resolution=self.config.resolution\n",
    "        )\n",
    "        \n",
    "        if self.config.subset_size:\n",
    "            dataset_obj = Subset(dataset_obj, indices=range(self.config.subset_size))\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset_obj,\n",
    "            batch_size=self.config.batch_size,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metric_scores = calculate_common_metric(\n",
    "            metrics=self.metrics,\n",
    "            dataloader=dataloader,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Store the result\n",
    "        self.result[dataset] = metric_scores\n",
    "        \n",
    "        # Print the metric result\n",
    "        print_metric_result(dataset, metric_scores)\n",
    "    \n",
    "    def evaluate_all_datasets(self):\n",
    "        \"\"\"\n",
    "        Iterates through all datasets and calculates metrics.\n",
    "        \"\"\"\n",
    "        datasets = self.get_datasets()\n",
    "        if not datasets:\n",
    "            print(colored(\"No datasets found in the real video base directory.\", \"red\"))\n",
    "            return\n",
    "        \n",
    "        for dataset in tqdm(datasets, desc=\"Processing Datasets\"):\n",
    "            self.calculate_metrics_for_dataset(dataset)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        self.calculate_average_metrics()\n",
    "        \n",
    "        # Save results to JSON\n",
    "        self.save_results()\n",
    "        \n",
    "        # Print average results\n",
    "        self.print_average_results()\n",
    "    \n",
    "    def calculate_average_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculates the average of each metric across all datasets.\n",
    "        \"\"\"\n",
    "        if not self.result:\n",
    "            print(colored(\"No results to average.\", \"red\"))\n",
    "            return\n",
    "        \n",
    "        metric_sums = {metric: 0.0 for metric in self.metrics}\n",
    "        metric_counts = {metric: 0 for metric in self.metrics}\n",
    "        \n",
    "        for dataset_metrics in self.result.values():\n",
    "            for metric, score in dataset_metrics.items():\n",
    "                metric_sums[metric] += score\n",
    "                metric_counts[metric] += 1\n",
    "        \n",
    "        self.avg_result = {metric: (metric_sums[metric] / metric_counts[metric] \n",
    "                                    if metric_counts[metric] > 0 else 0.0) \n",
    "                           for metric in self.metrics}\n",
    "    \n",
    "    def print_average_results(self):\n",
    "        \"\"\"\n",
    "        Prints the average metrics across all datasets with colored output.\n",
    "        \"\"\"\n",
    "        print(colored(\"\\nAverage Metrics Across All Datasets:\", \"cyan\", attrs=[\"bold\"]))\n",
    "        for metric, score in self.avg_result.items():\n",
    "            if metric.lower() in [\"ssim\", \"psnr\"]:\n",
    "                color = \"green\" if metric.lower() == \"ssim\" else \"yellow\"\n",
    "            elif metric.lower() in [\"lpips\", \"flolpips\"]:\n",
    "                color = \"magenta\"\n",
    "            else:\n",
    "                color = \"white\"\n",
    "            print(colored(f\"  {metric.upper()}: {score:.4f}\", color, attrs=[\"bold\"]))\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"\n",
    "        Saves the per-dataset and average metrics to a JSON file.\n",
    "        \"\"\"\n",
    "        output_dict = {\n",
    "            \"per_dataset\": self.result,\n",
    "            \"average\": self.avg_result\n",
    "        }\n",
    "        \n",
    "        os.makedirs(os.path.dirname(self.config.result_json_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.config.result_json_path, \"w\") as f:\n",
    "            json.dump(output_dict, f, indent=4)\n",
    "        \n",
    "        print(colored(f\"\\nResults saved to {self.config.result_json_path}\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config(\n",
    "    data_root=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench\",  # Replace with your actual data root path\n",
    "    model_name=\"cogvideox\",  # Name of your model\n",
    "    device=\"cuda:1\",  # or \"cpu\"\n",
    "    dtype=\"float16\",  # or \"bfloat16\"\n",
    "    metrics=[\"ssim\", \"psnr\", \"lpips\", \"flolpips\"],  # List of metrics to calculate\n",
    "    batch_size=2,\n",
    "    num_workers=4,\n",
    "    num_frames=100,\n",
    "    sample_rate=1,\n",
    "    resolution=128,\n",
    "    crop_size=None,\n",
    "    subset_size=None,\n",
    "    fvd_method=\"styleganv\",\n",
    "    output_json=\"result.json\"\n",
    ")\n",
    "\n",
    "# Initialize Evaluator\n",
    "evaluator = Evaluator(config)\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate_all_datasets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
