{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Cosmos-Tokenizer-CV4x8x8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb47747aff5144b5b2345ce420dbfa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a6d11ecbf1439ebbb4207868f19cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_config.yaml:   0%|          | 0.00/92.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf5bb57bd094818bdebd4fcb374d427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4085a0c3e3394908b59fe30fe203cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855fdfd5a5f84a269faae5586fad623c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceac77177b624aeb9c544541ddfb2c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "decoder.jit:   0%|          | 0.00/88.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931700f20ea14db99d67f2f90411c678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoder.jit:   0%|          | 0.00/61.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4cab690a414d96a71179cd3663888d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "autoencoder.jit:   0%|          | 0.00/149M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# * Download the models, we evaluate continuous 4x8x8 here\n",
    "\n",
    "from huggingface_hub import login, snapshot_download\n",
    "import os\n",
    "\n",
    "# login(token=\"<YOUR-HF-TOKEN>\", add_to_git_credential=True)\n",
    "model_names = [\n",
    "        # \"Cosmos-Tokenizer-CI8x8\",\n",
    "        # \"Cosmos-Tokenizer-CI16x16\",\n",
    "        \"Cosmos-Tokenizer-CV4x8x8\",\n",
    "        # \"Cosmos-Tokenizer-CV8x8x8\",\n",
    "        # \"Cosmos-Tokenizer-CV8x16x16\",\n",
    "        # \"Cosmos-Tokenizer-DI8x8\",\n",
    "        # \"Cosmos-Tokenizer-DI16x16\",\n",
    "        # \"Cosmos-Tokenizer-DV4x8x8\",\n",
    "        # \"Cosmos-Tokenizer-DV8x8x8\",\n",
    "        # \"Cosmos-Tokenizer-DV8x16x16\",\n",
    "]\n",
    "for model_name in model_names:\n",
    "    hf_repo = \"nvidia/\" + model_name\n",
    "    local_dir = \"/home/maij/fall_2024/sora3r/Pollux/tvae_bench/pretrained_models/\" + model_name\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    print(f\"downloading {model_name}...\")\n",
    "    snapshot_download(repo_id=hf_repo, local_dir=local_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/NVIDIA/Cosmos-Tokenizer.git\n",
    "cd Cosmos-Tokenizer\n",
    "apt-get install -y ffmpeg\n",
    "pip3 install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cosmos_tokenizer.video_lib import CausalVideoTokenizer\n",
    "\n",
    "model_name = \"Cosmos-Tokenizer-CV4x8x8\"\n",
    "model_root=\"/home/maij/fall_2024/sora3r/Pollux/tvae_bench/pretrained_models\"\n",
    "input_tensor = torch.randn(1, 3, 9, 512, 512).to('cuda').to(torch.bfloat16)  # [B, C, T, H, W]\n",
    "encoder = CausalVideoTokenizer(checkpoint_enc=f'{model_root}/{model_name}/encoder.jit')\n",
    "(latent,) = encoder.encode(input_tensor)\n",
    "torch.testing.assert_close(latent.shape, (1, 16, 3, 64, 64))\n",
    "\n",
    "# The input tensor can be reconstructed by the decoder as:\n",
    "decoder = CausalVideoTokenizer(checkpoint_dec=f'{model_root}/{model_name}/decoder.jit')\n",
    "reconstructed_tensor = decoder.decode(latent)\n",
    "torch.testing.assert_close(reconstructed_tensor.shape, input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Type\n",
    "import logging\n",
    "\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: Type,\n",
    "        model_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        batch_size: int = 1,\n",
    "        custom_batch_size: Dict[str, int] = None,\n",
    "        source_base: str = \"../resources/videos/\",\n",
    "        output_base: str = \"./output_videos/\",\n",
    "        model_type: str = \"general\"  # \"general\" or \"cosmos\"\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.batch_size = batch_size\n",
    "        self.custom_batch_size = custom_batch_size or {}\n",
    "        self.source_base = source_base\n",
    "        self.output_base = output_base\n",
    "        self.model_type = model_type\n",
    "\n",
    "# GeneralAutoEncoderKL Class\n",
    "class GeneralAutoEncoderKL:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dtype = self.config.dtype\n",
    "\n",
    "        os.makedirs(self.config.output_base, exist_ok=True)\n",
    "\n",
    "        if self.config.model_type == \"general\":\n",
    "            self.model = self.config.model_class.from_pretrained(\n",
    "                self.config.model_path,\n",
    "                torch_dtype=self.dtype\n",
    "            ).to(self.device)\n",
    "            self.model.enable_slicing()\n",
    "            self.model.enable_tiling()\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            from cosmos_tokenizer.video_lib import CausalVideoTokenizer\n",
    "            self.encoder = CausalVideoTokenizer(\n",
    "                checkpoint_enc=f'{self.config.model_path}/encoder.jit'\n",
    "            ).to(self.device)\n",
    "            self.decoder = CausalVideoTokenizer(\n",
    "                checkpoint_dec=f'{self.config.model_path}/decoder.jit'\n",
    "            ).to(self.device)\n",
    "\n",
    "        print(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def preprocess_videos(self, video_paths: List[str]) -> Tuple[torch.Tensor, List[float], List[int], List[Tuple[int, int]]]:\n",
    "        batch_frames = []\n",
    "        fps_list, num_frames_list, resolutions = [], [], []\n",
    "\n",
    "        for video_path in video_paths:\n",
    "            video_reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "            meta_data = video_reader.get_meta_data()\n",
    "            fps = meta_data.get('fps', 30)\n",
    "\n",
    "            frames = [self.transform(frame) for frame in video_reader]\n",
    "            video_reader.close()\n",
    "\n",
    "            if not frames:\n",
    "                raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "\n",
    "            num_frames = len(frames)\n",
    "            resolution = frames[0].shape[1], frames[0].shape[2]\n",
    "\n",
    "            fps_list.append(fps)\n",
    "            num_frames_list.append(num_frames)\n",
    "            resolutions.append(resolution)\n",
    "\n",
    "            frames_tensor = torch.stack(frames).to(self.device).permute(1, 0, 2, 3)\n",
    "            batch_frames.append(frames_tensor)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_frames).to(self.dtype)\n",
    "        return batch_tensor, fps_list, num_frames_list, resolutions\n",
    "\n",
    "    def encode(self, frames_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.config.model_type == \"general\":\n",
    "            with torch.no_grad():\n",
    "                encoded_frames = self.model.encode(frames_tensor)[0].sample()\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            (encoded_frames,) = self.encoder.encode(frames_tensor)\n",
    "        return encoded_frames\n",
    "\n",
    "    def decode(self, encoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.config.model_type == \"general\":\n",
    "            with torch.no_grad():\n",
    "                decoded_frames = self.model.decode(encoded_tensor).sample\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            decoded_frames = self.decoder.decode(encoded_tensor)\n",
    "        return decoded_frames\n",
    "\n",
    "    def save_videos(self, tensor: torch.Tensor, output_paths: List[str], fps_list: List[float],\n",
    "                    num_frames_list: List[int], resolutions: List[Tuple[int, int]]):\n",
    "        tensor = tensor.to(dtype=torch.float32)\n",
    "        for i, (fps, num_frames, resolution, output_path) in enumerate(zip(fps_list, num_frames_list, resolutions, output_paths)):\n",
    "            frames = tensor[i].permute(1, 2, 3, 0).cpu().numpy()\n",
    "            frames = np.clip(frames, 0, 1) * 255\n",
    "            frames = frames.astype(np.uint8)\n",
    "\n",
    "            num_output_frames = frames.shape[0]\n",
    "            assert num_output_frames == num_frames, (\n",
    "                f\"Frame count mismatch: input {num_frames} vs output {num_output_frames}\")\n",
    "\n",
    "            output_resolution = frames.shape[1], frames.shape[2]\n",
    "            assert output_resolution == resolution, (\n",
    "                f\"Resolution mismatch: input {resolution} vs output {output_resolution}\")\n",
    "\n",
    "            writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "            for frame in frames:\n",
    "                writer.append_data(frame)\n",
    "            writer.close()\n",
    "\n",
    "            print(f\"Saved video to {output_path} with {num_output_frames} frames at {output_resolution} resolution and {fps} fps.\")\n",
    "\n",
    "    def reconstruct_videos(self, video_paths: List[str], output_paths: List[str]):\n",
    "        batch_size = self.config.batch_size\n",
    "        for dataset_name, custom_size in self.config.custom_batch_size.items():\n",
    "            if any(dataset_name in path for path in video_paths):\n",
    "                batch_size = custom_size\n",
    "                break\n",
    "\n",
    "        for i in range(0, len(video_paths), batch_size):\n",
    "            batch_video_paths = video_paths[i:i + batch_size]\n",
    "            batch_output_paths = output_paths[i:i + batch_size]\n",
    "\n",
    "            frames_tensor, fps_list, num_frames_list, resolutions = self.preprocess_videos(batch_video_paths)\n",
    "            encoded = self.encode(frames_tensor)\n",
    "            decoded = self.decode(encoded)\n",
    "            self.save_videos(decoded, batch_output_paths, fps_list, num_frames_list, resolutions)\n",
    "            del frames_tensor, encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /home/maij/fall_2024/sora3r/Pollux/tvae_bench/pretrained_models/Cosmos-Tokenizer-CV4x8x8.\n",
      "Processing dataset: ego-exo-4d-ego\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cosmos/ego-exo-4d-ego/0000.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cosmos/ego-exo-4d-ego/0002.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cosmos/ego-exo-4d-ego/0004.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cosmos/ego-exo-4d-ego/0013.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cosmos/ego-exo-4d-ego/0015.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     video_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_source_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video_files]\n\u001b[1;32m     34\u001b[0m     output_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_output_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video_files]\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll datasets have been processed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mGeneralAutoEncoderKL.reconstruct_videos\u001b[0;34m(self, video_paths, output_paths)\u001b[0m\n\u001b[1;32m    143\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(frames_tensor)\n\u001b[1;32m    144\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(encoded)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m frames_tensor, encoded, decoded\n",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m, in \u001b[0;36mGeneralAutoEncoderKL.save_videos\u001b[0;34m(self, tensor, output_paths, fps_list, num_frames_list, resolutions)\u001b[0m\n\u001b[1;32m    124\u001b[0m writer \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mget_writer(output_path, fps\u001b[38;5;241m=\u001b[39mfps, codec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibx264\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames:\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved video to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_output_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m frames at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_resolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m resolution and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fps.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/sora/lib/python3.9/site-packages/imageio/core/format.py:590\u001b[0m, in \u001b[0;36mFormat.Writer.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    588\u001b[0m im \u001b[38;5;241m=\u001b[39m asarray(im)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Call\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/sora/lib/python3.9/site-packages/imageio/plugins/ffmpeg.py:577\u001b[0m, in \u001b[0;36mFfmpegFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# To be written efficiently, ie. without creating an immutable\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# buffer, by calling im.tobytes() the array must be contiguous.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m im\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# checkign the flag is a micro optimization.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;66;03m# the image will be a numpy subclass. See discussion\u001b[39;00m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/issues/11804\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Set size and initialize if not initialized yet\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# * Testing code on H100 server\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set Hugging Face home directory\n",
    "os.environ[\"HF_HOME\"] = \"/jfs/jinjie/huggingface\"\n",
    "def format_timedelta(td):\n",
    "    \"\"\"Formats a timedelta object into HH:MM:SS string.\"\"\"\n",
    "    seconds = int(td.total_seconds())\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "from cosmos_tokenizer.video_lib import CausalVideoTokenizer\n",
    "\n",
    "# Processing Script\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data_root=\"/jfs/jinjie\"\n",
    "    \n",
    "    config = Config(\n",
    "        model_class=None,  # Not needed for Cosmos\n",
    "        model_path=f\"{data_root}/huggingface/pretrained_models/Cosmos-Tokenizer-CV4x8x8\",   # Not needed for Cosmos\n",
    "        device=\"cuda:1\",\n",
    "        dtype='bfloat16',\n",
    "        batch_size=2,\n",
    "        custom_batch_size={'imagenet_val': 1, 'textocr': 1, 'bridgedata_v2':1, 'panda_70m':1, 'real10k':1}, # * variable resolution\n",
    "        source_base=f\"{data_root}/data/vae_eval_bench/processed_gt_v3\", \n",
    "        output_base=f\"{data_root}/data/vae_eval_bench/model_recon/cosmos\",\n",
    "        model_type=\"cosmos\",  # Specify Cosmos as the model type\n",
    "    )\n",
    "\n",
    "    autoencoder = GeneralAutoEncoderKL(config)\n",
    "\n",
    "    for dataset in os.listdir(config.source_base):\n",
    "        \n",
    "        dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "        dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "\n",
    "        if not os.path.isdir(dataset_source_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        start_time = datetime.now()  # Start the timer\n",
    "        \n",
    "        os.makedirs(dataset_output_path, exist_ok=True)\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(dataset_source_path) if f.endswith('.mp4')])\n",
    "        video_paths = [os.path.join(dataset_source_path, f) for f in video_files]\n",
    "        output_paths = [os.path.join(dataset_output_path, f) for f in video_files]\n",
    "\n",
    "        autoencoder.reconstruct_videos(video_paths, output_paths)\n",
    "        \n",
    "        end_time = datetime.now()  # Stop the timer\n",
    "        time_taken = end_time - start_time\n",
    "        formatted_time = format_timedelta(time_taken)\n",
    "\n",
    "        print(f\"Finished processing {dataset}. Time taken: {formatted_time}\")\n",
    "        \n",
    "\n",
    "    print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
