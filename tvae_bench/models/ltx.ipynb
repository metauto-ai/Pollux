{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d5b5dc6a514b0c9f2ac9fc1eb0e17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ltx-video-2b-v0.9.safetensors:   0%|          | 0.00/9.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455c1544768749468a1733761307e4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKLLTXVideo\n",
    "\n",
    "single_file_url = \"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\"\n",
    "# transformer = LTXVideoTransformer3DModel.from_single_file(\n",
    "#   single_file_url, torch_dtype=torch.bfloat16\n",
    "# )\n",
    "vae = AutoencoderKLLTXVideo.from_single_file(single_file_url, torch_dtype=torch.bfloat16)\n",
    "# pipe = LTXImageToVideoPipeline.from_pretrained(\n",
    "#   \"Lightricks/LTX-Video\", transformer=transformer, vae=vae, torch_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Type\n",
    "import logging\n",
    "\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: Type,\n",
    "        model_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        batch_size: int = 1,\n",
    "        custom_batch_size: Dict[str, int] = None,\n",
    "        source_base: str = \"../resources/videos/\",\n",
    "        output_base: str = \"./output_videos/\",\n",
    "        model_type: str = \"general\"  # \"general\" or \"cosmos\"\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.batch_size = batch_size\n",
    "        self.custom_batch_size = custom_batch_size or {}\n",
    "        self.source_base = source_base\n",
    "        self.output_base = output_base\n",
    "        self.model_type = model_type\n",
    "\n",
    "# GeneralAutoEncoderKL Class\n",
    "class GeneralAutoEncoderKL:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dtype = self.config.dtype\n",
    "\n",
    "        os.makedirs(self.config.output_base, exist_ok=True)\n",
    "\n",
    "        if self.config.model_type == \"general\":\n",
    "            self.model = self.config.model_class.from_pretrained(\n",
    "                self.config.model_path,\n",
    "                torch_dtype=self.dtype\n",
    "            ).to(self.device)\n",
    "            self.model.enable_slicing()\n",
    "            self.model.enable_tiling()\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            from cosmos_tokenizer.video_lib import CausalVideoTokenizer\n",
    "            self.encoder = CausalVideoTokenizer(\n",
    "                checkpoint_enc=f'{self.config.model_path}/encoder.jit'\n",
    "            ).to(self.device)\n",
    "            self.decoder = CausalVideoTokenizer(\n",
    "                checkpoint_dec=f'{self.config.model_path}/decoder.jit'\n",
    "            ).to(self.device)\n",
    "        elif self.config.model_type == \"ltx\":\n",
    "            self.model = self.config.model_class.from_single_file(\n",
    "                self.config.model_path,\n",
    "                torch_dtype=self.dtype\n",
    "            ).to(self.device)\n",
    "            # self.model.enable_slicing()\n",
    "            # self.model.enable_tiling()\n",
    "            self.model.disable_slicing()\n",
    "            self.model.disable_tiling()\n",
    "\n",
    "        print(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def preprocess_videos(self, video_paths: List[str]) -> Tuple[torch.Tensor, List[float], List[int], List[Tuple[int, int]]]:\n",
    "        batch_frames = []\n",
    "        fps_list, num_frames_list, resolutions = [], [], []\n",
    "\n",
    "        for video_path in video_paths:\n",
    "            video_reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "            meta_data = video_reader.get_meta_data()\n",
    "            fps = meta_data.get('fps', 30)\n",
    "\n",
    "            frames = [self.transform(frame) for frame in video_reader]\n",
    "            video_reader.close()\n",
    "\n",
    "            if not frames:\n",
    "                raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "\n",
    "            num_frames = len(frames)\n",
    "            resolution = frames[0].shape[1], frames[0].shape[2]\n",
    "\n",
    "            fps_list.append(fps)\n",
    "            num_frames_list.append(num_frames)\n",
    "            resolutions.append(resolution)\n",
    "\n",
    "            frames_tensor = torch.stack(frames).to(self.device).permute(1, 0, 2, 3)\n",
    "            batch_frames.append(frames_tensor)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_frames).to(self.dtype)\n",
    "        return batch_tensor, fps_list, num_frames_list, resolutions\n",
    "\n",
    "    def encode(self, frames_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.config.model_type in [\"general\",\"ltx\"]:\n",
    "            with torch.no_grad():\n",
    "                encoded_frames = self.model.encode(frames_tensor)[0].sample()\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            (encoded_frames,) = self.encoder.encode(frames_tensor)\n",
    "        return encoded_frames\n",
    "\n",
    "    def decode(self, encoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.config.model_type in [\"general\",\"ltx\"]:\n",
    "            with torch.no_grad():\n",
    "                decoded_frames = self.model.decode(encoded_tensor).sample\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            decoded_frames = self.decoder.decode(encoded_tensor)\n",
    "        return decoded_frames\n",
    "\n",
    "    def save_videos(self, tensor: torch.Tensor, output_paths: List[str], fps_list: List[float],\n",
    "                    num_frames_list: List[int], resolutions: List[Tuple[int, int]]):\n",
    "        tensor = tensor.to(dtype=torch.float32)\n",
    "        for i, (fps, num_frames, resolution, output_path) in enumerate(zip(fps_list, num_frames_list, resolutions, output_paths)):\n",
    "            frames = tensor[i].permute(1, 2, 3, 0).cpu().numpy()\n",
    "            frames = np.clip(frames, 0, 1) * 255\n",
    "            frames = frames.astype(np.uint8)\n",
    "\n",
    "            num_output_frames = frames.shape[0]\n",
    "            assert num_output_frames == num_frames, (\n",
    "                f\"Frame count mismatch: input {num_frames} vs output {num_output_frames}\")\n",
    "\n",
    "            output_resolution = frames.shape[1], frames.shape[2]\n",
    "            assert output_resolution == resolution, (\n",
    "                f\"Resolution mismatch: input {resolution} vs output {output_resolution}\")\n",
    "\n",
    "            writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "            for frame in frames:\n",
    "                writer.append_data(frame)\n",
    "            writer.close()\n",
    "\n",
    "            print(f\"Saved video to {output_path} with {num_output_frames} frames at {output_resolution} resolution and {fps} fps.\")\n",
    "\n",
    "    def reconstruct_videos(self, video_paths: List[str], output_paths: List[str]):\n",
    "        batch_size = self.config.batch_size\n",
    "        for dataset_name, custom_size in self.config.custom_batch_size.items():\n",
    "            if any(dataset_name in path for path in video_paths):\n",
    "                batch_size = custom_size\n",
    "                break\n",
    "\n",
    "        for i in range(0, len(video_paths), batch_size):\n",
    "            batch_video_paths = video_paths[i:i + batch_size]\n",
    "            batch_output_paths = output_paths[i:i + batch_size]\n",
    "\n",
    "            frames_tensor, fps_list, num_frames_list, resolutions = self.preprocess_videos(batch_video_paths)\n",
    "            encoded = self.encode(frames_tensor)\n",
    "            decoded = self.decode(encoded)\n",
    "            self.save_videos(decoded, batch_output_paths, fps_list, num_frames_list, resolutions)\n",
    "            del frames_tensor, encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors.\n",
      "Processing dataset: BDD100K\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Resolution mismatch: input (720, 1280) vs output (736, 1280)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m video_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_source_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video_files]\n\u001b[1;32m     55\u001b[0m output_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_output_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video_files]\n\u001b[0;32m---> 57\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()  \u001b[38;5;66;03m# Stop the timer\u001b[39;00m\n\u001b[1;32m     60\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mGeneralAutoEncoderKL.reconstruct_videos\u001b[0;34m(self, video_paths, output_paths)\u001b[0m\n\u001b[1;32m    152\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(frames_tensor)\n\u001b[1;32m    153\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(encoded)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m frames_tensor, encoded, decoded\n",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m, in \u001b[0;36mGeneralAutoEncoderKL.save_videos\u001b[0;34m(self, tensor, output_paths, fps_list, num_frames_list, resolutions)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_output_frames \u001b[38;5;241m==\u001b[39m num_frames, (\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame count mismatch: input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs output \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_output_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m output_resolution \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output_resolution \u001b[38;5;241m==\u001b[39m resolution, (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResolution mismatch: input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs output \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_resolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m writer \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mget_writer(output_path, fps\u001b[38;5;241m=\u001b[39mfps, codec\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibx264\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Resolution mismatch: input (720, 1280) vs output (736, 1280)"
     ]
    }
   ],
   "source": [
    "# * Testing code on H100 server\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set Hugging Face home directory\n",
    "os.environ[\"HF_HOME\"] = \"/jfs/jinjie/huggingface\"\n",
    "def format_timedelta(td):\n",
    "    \"\"\"Formats a timedelta object into HH:MM:SS string.\"\"\"\n",
    "    seconds = int(td.total_seconds())\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "\n",
    "from diffusers import AutoencoderKLLTXVideo\n",
    "\n",
    "# Processing Script\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data_root=\"/jfs/jinjie\"\n",
    "    \n",
    "    \n",
    "    config = Config(\n",
    "        model_class=AutoencoderKLLTXVideo,  \n",
    "        model_path=\"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\",   # they patch model into a single file\n",
    "        device=\"cuda:3\",\n",
    "        dtype='bfloat16',\n",
    "        batch_size=4,\n",
    "        custom_batch_size={'imagenet_val': 1, 'textocr': 1, 'bridgedata_v2':1, 'panda_70m':1, 'real10k':1}, # * variable resolution\n",
    "        source_base=f\"{data_root}/data/vae_eval_bench/processed_gt_v3\", \n",
    "        output_base=f\"{data_root}/data/vae_eval_bench/model_recon/ltx\",\n",
    "        model_type=\"ltx\",  # Specify Cosmos as the model type\n",
    "    )\n",
    "\n",
    "    autoencoder = GeneralAutoEncoderKL(config)\n",
    "    \n",
    "\n",
    "    for dataset in os.listdir(config.source_base):\n",
    "        \n",
    "        dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "        dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "\n",
    "        if not os.path.isdir(dataset_source_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        start_time = datetime.now()  # Start the timer\n",
    "        \n",
    "        \n",
    "        os.makedirs(dataset_output_path, exist_ok=True)\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(dataset_source_path) if f.endswith('.mp4')])\n",
    "        video_paths = [os.path.join(dataset_source_path, f) for f in video_files]\n",
    "        output_paths = [os.path.join(dataset_output_path, f) for f in video_files]\n",
    "\n",
    "        autoencoder.reconstruct_videos(video_paths, output_paths)\n",
    "        \n",
    "        end_time = datetime.now()  # Stop the timer\n",
    "        time_taken = end_time - start_time\n",
    "        formatted_time = format_timedelta(time_taken)\n",
    "\n",
    "        print(f\"Finished processing {dataset}. Time taken: {formatted_time}\")\n",
    "\n",
    "    print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "However, during my testing, I filtered out **LTX-V** and **Mochi-1**.\n",
    "\n",
    "For **LTX-V**, it has a serious problem that it can't support resolution invariant reconstruction. The test benchmark I constructed has many videos/images under many different resolutions, **LTX-V** can only preserve the resolution, that is, height x width unchanged for some specific resolutions.\n",
    "\n",
    "But even for the most common 720x1280 videos, it will truncate the reconstructed resolution to 704x1280.\n",
    "\n",
    "Considering that **LTX-V** is new and also not that popular in the community (its repo has ~2k stars and not that many people are using it), we omit it for evaluation for its resolution limitation.\n",
    "```\n",
    "Cell In[1], [line 128](vscode-notebook-cell:?execution_count=1&line=128)\n",
    "\n",
    "    [132](vscode-notebook-cell:?execution_count=1&line=132) for frame in frames:\n",
    "\n",
    "AssertionError: Resolution mismatch: input (720, 1280) vs output (704, 1280)\n",
    "```\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
