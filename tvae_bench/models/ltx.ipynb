{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d5b5dc6a514b0c9f2ac9fc1eb0e17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ltx-video-2b-v0.9.safetensors:   0%|          | 0.00/9.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455c1544768749468a1733761307e4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKLLTXVideo\n",
    "\n",
    "single_file_url = \"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\"\n",
    "# transformer = LTXVideoTransformer3DModel.from_single_file(\n",
    "#   single_file_url, torch_dtype=torch.bfloat16\n",
    "# )\n",
    "vae = AutoencoderKLLTXVideo.from_single_file(single_file_url, torch_dtype=torch.bfloat16)\n",
    "# pipe = LTXImageToVideoPipeline.from_pretrained(\n",
    "#   \"Lightricks/LTX-Video\", transformer=transformer, vae=vae, torch_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Type\n",
    "import logging\n",
    "\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: Type,\n",
    "        model_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        batch_size: int = 1,\n",
    "        custom_batch_size: Dict[str, int] = None,\n",
    "        source_base: str = \"../resources/videos/\",\n",
    "        output_base: str = \"./output_videos/\",\n",
    "        model_type: str = \"general\"  # \"general\" or \"cosmos\"\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.batch_size = batch_size\n",
    "        self.custom_batch_size = custom_batch_size or {}\n",
    "        self.source_base = source_base\n",
    "        self.output_base = output_base\n",
    "        self.model_type = model_type\n",
    "\n",
    "# GeneralAutoEncoderKL Class\n",
    "class GeneralAutoEncoderKL:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dtype = self.config.dtype\n",
    "\n",
    "        os.makedirs(self.config.output_base, exist_ok=True)\n",
    "\n",
    "        if self.config.model_type == \"general\":\n",
    "            self.model = self.config.model_class.from_pretrained(\n",
    "                self.config.model_path,\n",
    "                torch_dtype=self.dtype\n",
    "            ).to(self.device)\n",
    "            self.model.enable_slicing()\n",
    "            self.model.enable_tiling()\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            from cosmos_tokenizer.video_lib import CausalVideoTokenizer\n",
    "            self.encoder = CausalVideoTokenizer(\n",
    "                checkpoint_enc=f'{self.config.model_path}/encoder.jit'\n",
    "            ).to(self.device)\n",
    "            self.decoder = CausalVideoTokenizer(\n",
    "                checkpoint_dec=f'{self.config.model_path}/decoder.jit'\n",
    "            ).to(self.device)\n",
    "        elif self.config.model_type == \"ltx\":\n",
    "            self.model = self.config.model_class.from_single_file(\n",
    "                self.config.model_path,\n",
    "                torch_dtype=self.dtype\n",
    "            ).to(self.device)\n",
    "            self.model.enable_slicing()\n",
    "            self.model.enable_tiling()\n",
    "\n",
    "        print(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def preprocess_videos(self, video_paths: List[str]) -> Tuple[torch.Tensor, List[float], List[int], List[Tuple[int, int]]]:\n",
    "        batch_frames = []\n",
    "        fps_list, num_frames_list, resolutions = [], [], []\n",
    "\n",
    "        for video_path in video_paths:\n",
    "            video_reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "            meta_data = video_reader.get_meta_data()\n",
    "            fps = meta_data.get('fps', 30)\n",
    "\n",
    "            frames = [self.transform(frame) for frame in video_reader]\n",
    "            video_reader.close()\n",
    "\n",
    "            if not frames:\n",
    "                raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "\n",
    "            num_frames = len(frames)\n",
    "            resolution = frames[0].shape[1], frames[0].shape[2]\n",
    "\n",
    "            fps_list.append(fps)\n",
    "            num_frames_list.append(num_frames)\n",
    "            resolutions.append(resolution)\n",
    "\n",
    "            frames_tensor = torch.stack(frames).to(self.device).permute(1, 0, 2, 3)\n",
    "            batch_frames.append(frames_tensor)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_frames).to(self.dtype)\n",
    "        return batch_tensor, fps_list, num_frames_list, resolutions\n",
    "\n",
    "    def encode(self, frames_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.config.model_type in [\"general\",\"ltx\"]:\n",
    "            with torch.no_grad():\n",
    "                encoded_frames = self.model.encode(frames_tensor)[0].sample()\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            (encoded_frames,) = self.encoder.encode(frames_tensor)\n",
    "        return encoded_frames\n",
    "\n",
    "    def decode(self, encoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        if self.config.model_type in [\"general\",\"ltx\"]:\n",
    "            with torch.no_grad():\n",
    "                decoded_frames = self.model.decode(encoded_tensor).sample\n",
    "        elif self.config.model_type == \"cosmos\":\n",
    "            decoded_frames = self.decoder.decode(encoded_tensor)\n",
    "        return decoded_frames\n",
    "\n",
    "    def save_videos(self, tensor: torch.Tensor, output_paths: List[str], fps_list: List[float],\n",
    "                    num_frames_list: List[int], resolutions: List[Tuple[int, int]]):\n",
    "        tensor = tensor.to(dtype=torch.float32)\n",
    "        for i, (fps, num_frames, resolution, output_path) in enumerate(zip(fps_list, num_frames_list, resolutions, output_paths)):\n",
    "            frames = tensor[i].permute(1, 2, 3, 0).cpu().numpy()\n",
    "            frames = np.clip(frames, 0, 1) * 255\n",
    "            frames = frames.astype(np.uint8)\n",
    "\n",
    "            num_output_frames = frames.shape[0]\n",
    "            assert num_output_frames == num_frames, (\n",
    "                f\"Frame count mismatch: input {num_frames} vs output {num_output_frames}\")\n",
    "\n",
    "            output_resolution = frames.shape[1], frames.shape[2]\n",
    "            assert output_resolution == resolution, (\n",
    "                f\"Resolution mismatch: input {resolution} vs output {output_resolution}\")\n",
    "\n",
    "            writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "            for frame in frames:\n",
    "                writer.append_data(frame)\n",
    "            writer.close()\n",
    "\n",
    "            print(f\"Saved video to {output_path} with {num_output_frames} frames at {output_resolution} resolution and {fps} fps.\")\n",
    "\n",
    "    def reconstruct_videos(self, video_paths: List[str], output_paths: List[str]):\n",
    "        batch_size = self.config.batch_size\n",
    "        for dataset_name, custom_size in self.config.custom_batch_size.items():\n",
    "            if any(dataset_name in path for path in video_paths):\n",
    "                batch_size = custom_size\n",
    "                break\n",
    "\n",
    "        for i in range(0, len(video_paths), batch_size):\n",
    "            batch_video_paths = video_paths[i:i + batch_size]\n",
    "            batch_output_paths = output_paths[i:i + batch_size]\n",
    "\n",
    "            frames_tensor, fps_list, num_frames_list, resolutions = self.preprocess_videos(batch_video_paths)\n",
    "            encoded = self.encode(frames_tensor)\n",
    "            decoded = self.decode(encoded)\n",
    "            self.save_videos(decoded, batch_output_paths, fps_list, num_frames_list, resolutions)\n",
    "            del frames_tensor, encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors.\n",
      "Processing dataset: ego-exo-4d-ego\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/ltx/ego-exo-4d-ego/0000.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/ltx/ego-exo-4d-ego/0002.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/ltx/ego-exo-4d-ego/0004.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n",
      "Saved video to /home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/ltx/ego-exo-4d-ego/0013.mp4 with 105 frames at (448, 448) resolution and 30.0 fps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     video_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_source_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video_files]\n\u001b[1;32m     34\u001b[0m     output_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_output_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m video_files]\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll datasets have been processed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 152\u001b[0m, in \u001b[0;36mGeneralAutoEncoderKL.reconstruct_videos\u001b[0;34m(self, video_paths, output_paths)\u001b[0m\n\u001b[1;32m    150\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(frames_tensor)\n\u001b[1;32m    151\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(encoded)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m frames_tensor, encoded, decoded\n",
      "Cell \u001b[0;32mIn[8], line 119\u001b[0m, in \u001b[0;36mGeneralAutoEncoderKL.save_videos\u001b[0;34m(self, tensor, output_paths, fps_list, num_frames_list, resolutions)\u001b[0m\n\u001b[1;32m    117\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (fps, num_frames, resolution, output_path) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(fps_list, num_frames_list, resolutions, output_paths)):\n\u001b[0;32m--> 119\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    120\u001b[0m     frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(frames, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m    121\u001b[0m     frames \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing code on V100 server with Cosmos\n",
    "from diffusers import AutoencoderKLLTXVideo\n",
    "\n",
    "# Processing Script\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        model_class=AutoencoderKLLTXVideo,  \n",
    "        model_path=\"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\",   # they patch model into a single file\n",
    "        device=\"cuda:1\",\n",
    "        dtype='float16',\n",
    "        # \"bfloat16\",  # TODO Cosmos requires bfloat16\n",
    "        batch_size=1,\n",
    "        custom_batch_size={'imagenet_val': 1, 'textocr': 1, 'bridgedata_v2':1},  # Variable resolution\n",
    "        source_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/processed_gt_v3\",\n",
    "        output_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/ltx\",\n",
    "        model_type=\"ltx\",  # Specify Cosmos as the model type\n",
    "    )\n",
    "\n",
    "    autoencoder = GeneralAutoEncoderKL(config)\n",
    "\n",
    "    for dataset in os.listdir(config.source_base):\n",
    "        dataset=\"ego-exo-4d-ego\" # TODO: V100 can't work with large resolution above 520p\n",
    "        dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "        dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "\n",
    "        if not os.path.isdir(dataset_source_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        os.makedirs(dataset_output_path, exist_ok=True)\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(dataset_source_path) if f.endswith('.mp4')])\n",
    "        video_paths = [os.path.join(dataset_source_path, f) for f in video_files]\n",
    "        output_paths = [os.path.join(dataset_output_path, f) for f in video_files]\n",
    "\n",
    "        autoencoder.reconstruct_videos(video_paths, output_paths)\n",
    "\n",
    "    print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
