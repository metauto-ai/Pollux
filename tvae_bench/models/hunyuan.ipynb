{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "# * note : we need to use \n",
    "\n",
    "model_id = \"tencent/HunyuanVideo\"\n",
    "transformer = HunyuanVideoTransformer3DModel.from_pretrained(\n",
    "    model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16, revision='refs/pr/18'\n",
    ")\n",
    "pipe = HunyuanVideoPipeline.from_pretrained(model_id, transformer=transformer, revision='refs/pr/18', torch_dtype=torch.float16)\n",
    "\n",
    "pipe.vae.enable_tiling()\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "output = pipe(\n",
    "    prompt=\"A cat walks on the grass, realistic\",\n",
    "    height=320,\n",
    "    width=512,\n",
    "    num_frames=61,\n",
    "    num_inference_steps=30,\n",
    ").frames[0]\n",
    "\n",
    "# video frames is in [0,255]\n",
    "export_to_video(output, \"output.mp4\", fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31.0\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "print(diffusers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Type\n",
    "import logging\n",
    "\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: Type,\n",
    "        model_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        batch_size: int = 1,\n",
    "        custom_batch_size: Dict[str, int] = None,\n",
    "        source_base: str = \"../resources/videos/\",\n",
    "        output_base: str = \"./output_videos/\"\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.batch_size = batch_size\n",
    "        self.custom_batch_size = custom_batch_size or {}\n",
    "        self.source_base = source_base\n",
    "        self.output_base = output_base\n",
    "\n",
    "# GeneralAutoEncoderKL Class\n",
    "class GeneralAutoEncoderKL:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dtype = self.config.dtype\n",
    "\n",
    "        os.makedirs(self.config.output_base, exist_ok=True)\n",
    "\n",
    "        self.model = self.config.model_class.from_pretrained(\n",
    "            self.config.model_path,\n",
    "            torch_dtype=self.dtype\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "\n",
    "        self.model.enable_slicing()\n",
    "        self.model.enable_tiling()\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def preprocess_videos(self, video_paths: List[str]) -> Tuple[torch.Tensor, List[float], List[int], List[Tuple[int, int]]]:\n",
    "        batch_frames = []\n",
    "        fps_list, num_frames_list, resolutions = [], [], []\n",
    "\n",
    "        for video_path in video_paths:\n",
    "            video_reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "            meta_data = video_reader.get_meta_data()\n",
    "            fps = meta_data.get('fps', 30)\n",
    "\n",
    "            frames = [self.transform(frame) for frame in video_reader]\n",
    "            video_reader.close()\n",
    "\n",
    "            if not frames:\n",
    "                raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "\n",
    "            num_frames = len(frames)\n",
    "            resolution = frames[0].shape[1], frames[0].shape[2]\n",
    "\n",
    "            fps_list.append(fps)\n",
    "            num_frames_list.append(num_frames)\n",
    "            resolutions.append(resolution)\n",
    "\n",
    "            frames_tensor = torch.stack(frames).to(self.device).permute(1, 0, 2, 3)\n",
    "            batch_frames.append(frames_tensor)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_frames).to(self.dtype)\n",
    "        return batch_tensor, fps_list, num_frames_list, resolutions\n",
    "\n",
    "    def encode(self, frames_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = self.model.encode(frames_tensor)[0].sample()\n",
    "        return encoded_frames\n",
    "\n",
    "    def decode(self, encoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            decoded_frames = self.model.decode(encoded_tensor).sample\n",
    "        return decoded_frames\n",
    "\n",
    "    def save_videos(self, tensor: torch.Tensor, output_paths: List[str], fps_list: List[float],\n",
    "                    num_frames_list: List[int], resolutions: List[Tuple[int, int]]):\n",
    "        tensor = tensor.to(dtype=torch.float32)\n",
    "        for i, (fps, num_frames, resolution, output_path) in enumerate(zip(fps_list, num_frames_list, resolutions, output_paths)):\n",
    "            frames = tensor[i].permute(1, 2, 3, 0).cpu().numpy()\n",
    "            frames = np.clip(frames, 0, 1) * 255\n",
    "            frames = frames.astype(np.uint8)\n",
    "\n",
    "            num_output_frames = frames.shape[0]\n",
    "            assert num_output_frames == num_frames, (\n",
    "                f\"Frame count mismatch: input {num_frames} vs output {num_output_frames}\")\n",
    "\n",
    "            output_resolution = frames.shape[1], frames.shape[2]\n",
    "            assert output_resolution == resolution, (\n",
    "                f\"Resolution mismatch: input {resolution} vs output {output_resolution}\")\n",
    "\n",
    "            writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "            for frame in frames:\n",
    "                writer.append_data(frame)\n",
    "            writer.close()\n",
    "\n",
    "            print(f\"Saved video to {output_path} with {num_output_frames} frames at {output_resolution} resolution and {fps} fps.\")\n",
    "\n",
    "    def reconstruct_videos(self, video_paths: List[str], output_paths: List[str]):\n",
    "        batch_size = self.config.batch_size\n",
    "        for dataset_name, custom_size in self.config.custom_batch_size.items():\n",
    "            if any(dataset_name in path for path in video_paths):\n",
    "                batch_size = custom_size\n",
    "                break\n",
    "\n",
    "        for i in range(0, len(video_paths), batch_size):\n",
    "            batch_video_paths = video_paths[i:i + batch_size]\n",
    "            batch_output_paths = output_paths[i:i + batch_size]\n",
    "\n",
    "            frames_tensor, fps_list, num_frames_list, resolutions = self.preprocess_videos(batch_video_paths)\n",
    "            encoded = self.encode(frames_tensor)\n",
    "            decoded = self.decode(encoded)\n",
    "            self.save_videos(decoded, batch_output_paths, fps_list, num_frames_list, resolutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Testing code on V100 server\n",
    "from diffusers import AutoencoderKLHunyuanVideo\n",
    "\n",
    "\n",
    "# Processing Script\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        model_class=AutoencoderKLHunyuanVideo,\n",
    "        # model_path=\"THUDM/CogVideoX-2b\",\n",
    "        # model_path=\"/home/maij/.cache/huggingface/hub/models--THUDM--CogVideoX-5b/snapshots/8d6ea3f817438460b25595a120f109b88d5fdfad/vae\",\n",
    "        model_path=\"/home/maij/.cache/huggingface/hub/models--tencent--HunyuanVideo/snapshots/2a15b5574ee77888e51ae6f593b2ceed8ce813e5/vae\",\n",
    "        device=\"cuda:0\",\n",
    "        dtype=\"bfloat16\", # TODO: testing on bloat16\n",
    "        batch_size=10,\n",
    "        custom_batch_size={'imagenet_val': 1, 'textocr': 1}, # * variable resolution\n",
    "        source_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/processed_gt_v3\", \n",
    "        output_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/hunyuan\"\n",
    "        # source_base=\"/mnt/data/jinjie/data/vae_eval_bench/processed_gt_v3\",\n",
    "        # output_base=\"/mnt/data/jinjie/data/vae_eval_bench/model_recon/cogvideox\"\n",
    "    )\n",
    "    \n",
    "    autoencoder = GeneralAutoEncoderKL(config)\n",
    "\n",
    "    for dataset in os.listdir(config.source_base):\n",
    "        dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "        dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "\n",
    "        if not os.path.isdir(dataset_source_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        os.makedirs(dataset_output_path, exist_ok=True)\n",
    "\n",
    "        video_files = [f for f in os.listdir(dataset_source_path) if f.endswith('.mp4')]\n",
    "        video_paths = [os.path.join(dataset_source_path, f) for f in video_files]\n",
    "        output_paths = [os.path.join(dataset_output_path, f) for f in video_files]\n",
    "\n",
    "        autoencoder.reconstruct_videos(video_paths, output_paths)\n",
    "\n",
    "    print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
