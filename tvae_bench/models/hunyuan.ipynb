{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "# * note : we need to use \n",
    "\n",
    "model_id = \"tencent/HunyuanVideo\"\n",
    "transformer = HunyuanVideoTransformer3DModel.from_pretrained(\n",
    "    model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16, revision='refs/pr/18'\n",
    ")\n",
    "pipe = HunyuanVideoPipeline.from_pretrained(model_id, transformer=transformer, revision='refs/pr/18', torch_dtype=torch.float16)\n",
    "\n",
    "pipe.vae.enable_tiling()\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "output = pipe(\n",
    "    prompt=\"A cat walks on the grass, realistic\",\n",
    "    height=320,\n",
    "    width=512,\n",
    "    num_frames=61,\n",
    "    num_inference_steps=30,\n",
    ").frames[0]\n",
    "\n",
    "# video frames is in [0,255]\n",
    "export_to_video(output, \"output.mp4\", fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31.0\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "print(diffusers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "# from diffusers import AutoencoderKLHunyuan\n",
    "from diffusers import AutoencoderKLHunyuanVideo\n",
    "from torchvision import transforms\n",
    "from typing import Tuple\n",
    "import logging\n",
    "\n",
    "# Suppress specific imageio FFmpeg warnings\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class to manage parameters for Hunyuan operations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        source_base: str = \"../resources/videos/\",\n",
    "        output_base: str = \"./output_videos/\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the configuration with default or specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - model_path (str): Path to the Hunyuan model.\n",
    "        - device (str): Computation device ('cuda' or 'cpu').\n",
    "        - dtype (str): Data type for computation ('float16' or 'bfloat16').\n",
    "        - source_base (str): Path to the base folder containing source datasets.\n",
    "        - output_base (str): Path to the base folder where output datasets will be saved.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.source_base = source_base\n",
    "        self.output_base = output_base\n",
    "\n",
    "# Hunyuan Class\n",
    "class Hunyuan:\n",
    "    \"\"\"\n",
    "    A class to handle encoding and decoding of videos using the Hunyuan-2b VAE model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        \"\"\"\n",
    "        Initializes the Hunyuan model based on the provided configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        - config (Config): Configuration object containing necessary parameters.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dtype = self.config.dtype\n",
    "        \n",
    "        # Ensure output base directory exists\n",
    "        os.makedirs(self.config.output_base, exist_ok=True)\n",
    "        \n",
    "        # Load the pre-trained model\n",
    "        self.model = AutoencoderKLHunyuanVideo.from_pretrained(\n",
    "            self.config.model_path, \n",
    "            torch_dtype=self.dtype\n",
    "        ).to(self.device)\n",
    "        print(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "        \n",
    "        # Enable optimizations\n",
    "        self.model.enable_slicing()\n",
    "        self.model.enable_tiling()\n",
    "        \n",
    "        # Define transformation\n",
    "        self.transform = transforms.ToTensor()\n",
    "    \n",
    "    def preprocess_video(self, video_path: str) -> Tuple[torch.Tensor, float, int, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Reads a video file and converts it into a tensor, extracting metadata.\n",
    "        \n",
    "        Parameters:\n",
    "        - video_path (str): Path to the video file.\n",
    "        \n",
    "        Returns:\n",
    "        - Tuple[torch.Tensor, float, int, Tuple[int, int]]: \n",
    "            - Preprocessed video tensor of shape (B, C, T, H, W).\n",
    "            - Frames per second (fps) of the input video.\n",
    "            - Number of frames in the input video.\n",
    "            - Resolution (height, width) of the input video.\n",
    "        \"\"\"\n",
    "        video_reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "        meta_data = video_reader.get_meta_data()\n",
    "        fps = meta_data.get('fps', 30)  # Default to 30 if fps not found\n",
    "        \n",
    "        frames = [self.transform(frame) for frame in video_reader]\n",
    "        video_reader.close()\n",
    "        \n",
    "        if not frames:\n",
    "            raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "        \n",
    "        num_frames = len(frames)\n",
    "        resolution = frames[0].shape[1], frames[0].shape[2]  # (Height, Width)\n",
    "        \n",
    "        frames_tensor = torch.stack(frames).to(self.device).permute(1, 0, 2, 3).unsqueeze(0).to(self.dtype)\n",
    "        return frames_tensor, fps, num_frames, resolution\n",
    "    \n",
    "    def encode(self, frames_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes video frames into latent representations.\n",
    "        \n",
    "        Parameters:\n",
    "        - frames_tensor (torch.Tensor): Video frames tensor.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Encoded latent tensor.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = self.model.encode(frames_tensor)[0].sample()\n",
    "        return encoded_frames\n",
    "    \n",
    "    def decode(self, encoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decodes latent representations back into video frames.\n",
    "        \n",
    "        Parameters:\n",
    "        - encoded_tensor (torch.Tensor): Encoded latent tensor.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Decoded video frames tensor.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            decoded_frames = self.model.decode(encoded_tensor).sample\n",
    "        return decoded_frames\n",
    "    \n",
    "    def save_video(self, tensor: torch.Tensor, output_path: str, fps: float, original_num_frames: int, resolution: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        Saves the decoded video frames to a video file and checks frame consistency.\n",
    "        \n",
    "        Parameters:\n",
    "        - tensor (torch.Tensor): Decoded video frames tensor.\n",
    "        - output_path (str): Path to save the output video.\n",
    "        - fps (float): Frames per second for the output video.\n",
    "        - original_num_frames (int): Number of frames in the input video.\n",
    "        - resolution (Tuple[int, int]): Resolution (height, width) of the input video.\n",
    "        \"\"\"\n",
    "        tensor = tensor.to(dtype=torch.float32)\n",
    "        frames = tensor[0].permute(1, 2, 3, 0).cpu().numpy()  # (T, H, W, C)\n",
    "        frames = np.clip(frames, 0, 1) * 255\n",
    "        frames = frames.astype(np.uint8)\n",
    "        \n",
    "        num_output_frames = frames.shape[0]\n",
    "        assert num_output_frames == original_num_frames, (\n",
    "            f\"Frame count mismatch: input {original_num_frames} vs output {num_output_frames}\"\n",
    "        )\n",
    "        \n",
    "        # Check if resolution matches\n",
    "        output_resolution = frames.shape[1], frames.shape[2]\n",
    "        assert output_resolution == resolution, (\n",
    "            f\"Resolution mismatch: input {resolution} vs output {output_resolution}\"\n",
    "        )\n",
    "        \n",
    "        writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "        writer.close()\n",
    "        print(f\"Saved decoded video to {output_path} with {num_output_frames} frames at {output_resolution} resolution and {fps} fps.\")\n",
    "    \n",
    "    def reconstruct_video(self, video_path: str, output_path: str, idx: int):\n",
    "        \"\"\"\n",
    "        Encodes and decodes a single video, then saves the reconstructed video.\n",
    "        Supports only .mp4 videos.\n",
    "        \n",
    "        Parameters:\n",
    "        - video_path (str): Path to the source video.\n",
    "        - output_path (str): Path to save the reconstructed video.\n",
    "        \"\"\"\n",
    "        file_extension = os.path.splitext(video_path)[1].lower()\n",
    "        video_extensions = ['.mp4']\n",
    "        \n",
    "        if file_extension in video_extensions:\n",
    "            frames_tensor, fps, num_frames, resolution = self.preprocess_video(video_path)\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"Processing {idx} Video: {video_path} | FPS: {fps} | Frames: {num_frames} | Resolution: {resolution}\")\n",
    "            encoded = self.encode(frames_tensor)\n",
    "            decoded = self.decode(encoded)\n",
    "            self.save_video(decoded, output_path, fps, num_frames, resolution)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_extension}. Skipping file: {video_path}\")\n",
    "    \n",
    "    def reconstruct_folder_videos(self, source_folder: str, output_folder: str):\n",
    "        \"\"\"\n",
    "        Processes all videos in the source folder by encoding and decoding them,\n",
    "        then saves the reconstructed videos to the output folder.\n",
    "        \n",
    "        Parameters:\n",
    "        - source_folder (str): Path to the folder containing source videos.\n",
    "        - output_folder (str): Path to the folder where reconstructed videos will be saved.\n",
    "        \"\"\"\n",
    "        supported_video_extensions = ['.mp4']\n",
    "        \n",
    "        videos = [f for f in os.listdir(source_folder) \n",
    "                  if os.path.splitext(f)[1].lower() in supported_video_extensions]\n",
    "        \n",
    "        # Sort the videos lexicographically (works correctly for zero-padded filenames)\n",
    "        videos = sorted(videos)\n",
    "        \n",
    "        if not videos:\n",
    "            print(f\"No supported videos found in source folder: {source_folder}\")\n",
    "            return\n",
    "        \n",
    "        for idx, video in enumerate(videos, 1):\n",
    "            source_video_path = os.path.join(source_folder, video)\n",
    "            output_video_path = os.path.join(output_folder, video)\n",
    "            self.reconstruct_video(source_video_path, output_video_path, idx)\n",
    "            \n",
    "        print(\"Finished reconstructing all videos in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processing Script Outside the Hunyuan Class\n",
    "# This script iterates through each dataset folder and processes the videos accordingly.\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config(\n",
    "    model_path=\"/home/maij/.cache/huggingface/hub/models--tencent--HunyuanVideo/snapshots/2a15b5574ee77888e51ae6f593b2ceed8ce813e5/vae\",  # Replace with your actual model path\n",
    "    device=\"cuda:2\",  # or \"cpu\"\n",
    "    # TODO : we should report bfloat16 result, on v100 test we use fp16 for now\n",
    "    dtype=\"float16\",  # or \"bfloat16\"\n",
    "    source_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/processed_gt_v3\",  # Replace with your actual source base folder path\n",
    "    output_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/hunyuan\"  # Replace with your desired output base folder path\n",
    ")\n",
    "\n",
    "# Initialize Hunyuan\n",
    "cog_video = Hunyuan(config)\n",
    "\n",
    "# Iterate through each dataset folder in the source base directory\n",
    "for dataset in os.listdir(config.source_base):\n",
    "    dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "    dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if not os.path.isdir(dataset_source_path):\n",
    "        continue  # Skip if not a directory\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    \n",
    "    # Ensure the output dataset directory exists\n",
    "    os.makedirs(dataset_output_path, exist_ok=True)\n",
    "    \n",
    "    # Reconstruct all videos in the current dataset folder\n",
    "    cog_video.reconstruct_folder_videos(dataset_source_path, dataset_output_path)\n",
    "    \n",
    "print(\"All datasets have been processed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
