{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Type\n",
    "import logging\n",
    "\n",
    "logging.getLogger('imageio_ffmpeg').setLevel(logging.ERROR)\n",
    "\n",
    "# Configuration Class\n",
    "class Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: Type,\n",
    "        model_path: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"float16\",\n",
    "        batch_size: int = 1,\n",
    "        custom_batch_size: Dict[str, int] = None,\n",
    "        source_base: str = \"../resources/videos/\",\n",
    "        output_base: str = \"./output_videos/\"\n",
    "    ):\n",
    "        self.model_class = model_class\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
    "        self.batch_size = batch_size\n",
    "        self.custom_batch_size = custom_batch_size or {}\n",
    "        self.source_base = source_base\n",
    "        self.output_base = output_base\n",
    "\n",
    "# GeneralAutoEncoderKL Class\n",
    "class GeneralAutoEncoderKL:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.device)\n",
    "        self.dtype = self.config.dtype\n",
    "\n",
    "        os.makedirs(self.config.output_base, exist_ok=True)\n",
    "\n",
    "        self.model = self.config.model_class.from_pretrained(\n",
    "            self.config.model_path,\n",
    "            torch_dtype=self.dtype\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(f\"Model loaded successfully from {self.config.model_path}.\")\n",
    "\n",
    "        self.model.enable_slicing()\n",
    "        self.model.enable_tiling()\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def preprocess_videos(self, video_paths: List[str]) -> Tuple[torch.Tensor, List[float], List[int], List[Tuple[int, int]]]:\n",
    "        batch_frames = []\n",
    "        fps_list, num_frames_list, resolutions = [], [], []\n",
    "\n",
    "        for video_path in video_paths:\n",
    "            video_reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "            meta_data = video_reader.get_meta_data()\n",
    "            fps = meta_data.get('fps', 30)\n",
    "\n",
    "            frames = [self.transform(frame) for frame in video_reader]\n",
    "            video_reader.close()\n",
    "\n",
    "            if not frames:\n",
    "                raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "\n",
    "            num_frames = len(frames)\n",
    "            resolution = frames[0].shape[1], frames[0].shape[2]\n",
    "\n",
    "            fps_list.append(fps)\n",
    "            num_frames_list.append(num_frames)\n",
    "            resolutions.append(resolution)\n",
    "\n",
    "            frames_tensor = torch.stack(frames).to(self.device).permute(1, 0, 2, 3)\n",
    "            batch_frames.append(frames_tensor)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_frames).to(self.dtype)\n",
    "        return batch_tensor, fps_list, num_frames_list, resolutions\n",
    "\n",
    "    def encode(self, frames_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = self.model.encode(frames_tensor)[0].sample()\n",
    "        return encoded_frames\n",
    "\n",
    "    def decode(self, encoded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            decoded_frames = self.model.decode(encoded_tensor).sample\n",
    "        return decoded_frames\n",
    "\n",
    "    def save_videos(self, tensor: torch.Tensor, output_paths: List[str], fps_list: List[float],\n",
    "                    num_frames_list: List[int], resolutions: List[Tuple[int, int]]):\n",
    "        tensor = tensor.to(dtype=torch.float32)\n",
    "        for i, (fps, num_frames, resolution, output_path) in enumerate(zip(fps_list, num_frames_list, resolutions, output_paths)):\n",
    "            frames = tensor[i].permute(1, 2, 3, 0).cpu().numpy()\n",
    "            frames = np.clip(frames, 0, 1) * 255\n",
    "            frames = frames.astype(np.uint8)\n",
    "\n",
    "            num_output_frames = frames.shape[0]\n",
    "            assert num_output_frames == num_frames, (\n",
    "                f\"Frame count mismatch: input {num_frames} vs output {num_output_frames}\")\n",
    "\n",
    "            output_resolution = frames.shape[1], frames.shape[2]\n",
    "            assert output_resolution == resolution, (\n",
    "                f\"Resolution mismatch: input {resolution} vs output {output_resolution}\")\n",
    "\n",
    "            writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "            for frame in frames:\n",
    "                writer.append_data(frame)\n",
    "            writer.close()\n",
    "\n",
    "            print(f\"Saved video to {output_path} with {num_output_frames} frames at {output_resolution} resolution and {fps} fps.\")\n",
    "\n",
    "    def reconstruct_videos(self, video_paths: List[str], output_paths: List[str]):\n",
    "        batch_size = self.config.batch_size\n",
    "        for dataset_name, custom_size in self.config.custom_batch_size.items():\n",
    "            if any(dataset_name in path for path in video_paths):\n",
    "                batch_size = custom_size\n",
    "                break\n",
    "\n",
    "        for i in range(0, len(video_paths), batch_size):\n",
    "            batch_video_paths = video_paths[i:i + batch_size]\n",
    "            batch_output_paths = output_paths[i:i + batch_size]\n",
    "\n",
    "            frames_tensor, fps_list, num_frames_list, resolutions = self.preprocess_videos(batch_video_paths)\n",
    "            encoded = self.encode(frames_tensor)\n",
    "            decoded = self.decode(encoded)\n",
    "            self.save_videos(decoded, batch_output_paths, fps_list, num_frames_list, resolutions)\n",
    "            del frames_tensor, encoded, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Testing code on H100 server\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set Hugging Face home directory\n",
    "os.environ[\"HF_HOME\"] = \"/jfs/jinjie/huggingface\"\n",
    "def format_timedelta(td):\n",
    "    \"\"\"Formats a timedelta object into HH:MM:SS string.\"\"\"\n",
    "    seconds = int(td.total_seconds())\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "\n",
    "\n",
    "from diffusers import AutoencoderKLCogVideoX\n",
    "\n",
    "\n",
    "# Processing Script\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    data_root=\"/jfs/jinjie\"\n",
    "    \n",
    "    \n",
    "    config = Config(\n",
    "        model_class=AutoencoderKLCogVideoX,\n",
    "        # model_path=\"THUDM/CogVideoX-2b\",\n",
    "        model_path=f\"{data_root}/huggingface/hub/models--THUDM--CogVideoX-2b/snapshots/1137dacfc2c9c012bed6a0793f4ecf2ca8e7ba01/vae\",\n",
    "        device=\"cuda:0\",\n",
    "        dtype=\"bfloat16\", \n",
    "        batch_size=10,\n",
    "        custom_batch_size={'imagenet_val': 1, 'textocr': 1, 'bridgedata_v2':1, 'panda_70m':1, 'real10k':1}, # * variable resolution\n",
    "        source_base=f\"{data_root}/data/vae_eval_bench/processed_gt_v3\", \n",
    "        output_base=f\"{data_root}/data/vae_eval_bench/model_recon/cogvideox\"\n",
    "        # source_base=\"/mnt/data/jinjie/data/vae_eval_bench/processed_gt_v3\",\n",
    "        # output_base=\"/mnt/data/jinjie/data/vae_eval_bench/model_recon/cogvideox\"\n",
    "    )\n",
    "\n",
    "    autoencoder = GeneralAutoEncoderKL(config)\n",
    "\n",
    "    for dataset in os.listdir(config.source_base):\n",
    "\n",
    "        dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "        dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "\n",
    "        if not os.path.isdir(dataset_source_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        start_time = datetime.now()  # Start the timer\n",
    "        \n",
    "        os.makedirs(dataset_output_path, exist_ok=True)\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(dataset_source_path) if f.endswith('.mp4')])\n",
    "        video_paths = [os.path.join(dataset_source_path, f) for f in video_files]\n",
    "        output_paths = [os.path.join(dataset_output_path, f) for f in video_files]\n",
    "\n",
    "        autoencoder.reconstruct_videos(video_paths, output_paths)\n",
    "        \n",
    "        end_time = datetime.now()  # Stop the timer\n",
    "        time_taken = end_time - start_time\n",
    "        formatted_time = format_timedelta(time_taken)\n",
    "\n",
    "        print(f\"Finished processing {dataset}. Time taken: {formatted_time}\")\n",
    "\n",
    "\n",
    "    print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Testing code on V100 server\n",
    "from diffusers import AutoencoderKLCogVideoX\n",
    "\n",
    "\n",
    "# Processing Script\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        model_class=AutoencoderKLCogVideoX,\n",
    "        # model_path=\"THUDM/CogVideoX-2b\",\n",
    "        model_path=\"/home/maij/.cache/huggingface/hub/models--THUDM--CogVideoX-5b/snapshots/8d6ea3f817438460b25595a120f109b88d5fdfad/vae\",\n",
    "        device=\"cuda:0\",\n",
    "        dtype=\"float16\", # TODO: testing on bloat16\n",
    "        batch_size=4,\n",
    "        custom_batch_size={'imagenet_val': 1, 'textocr': 1, 'bridgedata_v2':1, 'panda_70m':1}, # * variable resolution\n",
    "        source_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/processed_gt_v3\", \n",
    "        output_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cogvideox\"\n",
    "        # source_base=\"/mnt/data/jinjie/data/vae_eval_bench/processed_gt_v3\",\n",
    "        # output_base=\"/mnt/data/jinjie/data/vae_eval_bench/model_recon/cogvideox\"\n",
    "    )\n",
    "\n",
    "    autoencoder = GeneralAutoEncoderKL(config)\n",
    "\n",
    "    for dataset in os.listdir(config.source_base):\n",
    "        if dataset in ['BDD100K','bridgedata_v2','imagenet_val','objaverse','ego-exo-4d-ego','dynamic_replica','textocr']:\n",
    "            continue\n",
    "        dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "        dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "\n",
    "        if not os.path.isdir(dataset_source_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        os.makedirs(dataset_output_path, exist_ok=True)\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(dataset_source_path) if f.endswith('.mp4')])\n",
    "        video_paths = [os.path.join(dataset_source_path, f) for f in video_files]\n",
    "        output_paths = [os.path.join(dataset_output_path, f) for f in video_files]\n",
    "\n",
    "        autoencoder.reconstruct_videos(video_paths, output_paths)\n",
    "\n",
    "    print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processing Script Outside the CogVideoX Class\n",
    "# This script iterates through each dataset folder and processes the videos accordingly.\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config(\n",
    "    model_path=\"/home/maij/.cache/huggingface/hub/models--THUDM--CogVideoX-5b/snapshots/8d6ea3f817438460b25595a120f109b88d5fdfad/vae\",  # Replace with your actual model path\n",
    "    device=\"cuda:1\",  # or \"cpu\"\n",
    "    # TODO : we should report bfloat16 result, on v100 test we use fp16 for now\n",
    "    dtype=\"float16\",  # or \"bfloat16\"\n",
    "    source_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/processed_gt_v2\",  # Replace with your actual source base folder path\n",
    "    output_base=\"/home/maij/fall_2024/sora3r/Open-Sora/data/vae_eval_bench/model_recon/cogvideox\"  # Replace with your desired output base folder path\n",
    ")\n",
    "\n",
    "# Initialize CogVideoX\n",
    "cog_video = CogVideoX(config)\n",
    "\n",
    "# Iterate through each dataset folder in the source base directory\n",
    "for dataset in os.listdir(config.source_base):\n",
    "    dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "    dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if not os.path.isdir(dataset_source_path):\n",
    "        continue  # Skip if not a directory\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    \n",
    "    # Ensure the output dataset directory exists\n",
    "    os.makedirs(dataset_output_path, exist_ok=True)\n",
    "    \n",
    "    # Reconstruct all videos in the current dataset folder\n",
    "    cog_video.reconstruct_folder_videos(dataset_source_path, dataset_output_path)\n",
    "    \n",
    "print(\"All datasets have been processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * configuration on A100 node\n",
    "\n",
    "# THUDM/CogVideoX-2b\n",
    "\n",
    "# Processing Script Outside the CogVideoX Class\n",
    "# This script iterates through each dataset folder and processes the videos accordingly.\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config(\n",
    "    model_path=\"THUDM/CogVideoX-2b\",  # Replace with your actual model path\n",
    "    device=\"cuda:0\",  # or \"cpu\"\n",
    "    dtype=\"bfloat16\",  # or \"bfloat16\"\n",
    "    source_base=\"/mnt/data/jinjie/data/vae_eval_bench/processed_gt_v2\",  # Replace with your actual source base folder path\n",
    "    output_base=\"/mnt/data/jinjie/data/vae_eval_bench/model_recon/cogvideox\"  # Replace with your desired output base folder path\n",
    ")\n",
    "\n",
    "# Initialize CogVideoX\n",
    "cog_video = CogVideoX(config)\n",
    "\n",
    "# Iterate through each dataset folder in the source base directory\n",
    "for dataset in os.listdir(config.source_base):\n",
    "    dataset_source_path = os.path.join(config.source_base, dataset)\n",
    "    dataset_output_path = os.path.join(config.output_base, dataset)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if not os.path.isdir(dataset_source_path):\n",
    "        continue  # Skip if not a directory\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    \n",
    "    # Ensure the output dataset directory exists\n",
    "    os.makedirs(dataset_output_path, exist_ok=True)\n",
    "    \n",
    "    # Reconstruct all videos in the current dataset folder\n",
    "    cog_video.reconstruct_folder_videos(dataset_source_path, dataset_output_path)\n",
    "    \n",
    "print(\"All datasets have been processed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
