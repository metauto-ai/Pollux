# torchrun --standalone --nnodes 1 --nproc-per-node 2 -m apps.main.train config=apps/main/configs/train_bucket_3840_image_from_pretrain.yaml

# Set up single experiment
version: v1.0
# From now on, start to align train, data, and model setting with train stage (just finish refactor for dara)
train_stage: stage-1 # options: preliminary, pretraining, posttraining; aligned with data setting
name: Pollux_v1.0_pre-trained_token_drop-added-extended-tokens-new  #used for local dump and wandb log
output_dir: /mnt/pollux/checkpoints/dump/
dump_dir: '' # No need now
steps: 500000
seed: 777
optim:
    lr: 1e-4
    warmup: 4000
    lr_min_ratio: 1.5e-5
    clip: 1.0
    weight_decay: 0.01

distributed:
    gpus: 2,3
    fsdp_type: full_shard
    dp_shard: 2
    dp_replicate: 1
    compile: false
    model_dtype: bf16 # options: `fb8` is only supported by H100
    matmul_allow_tf32: false
    selective_activation_checkpointing: false
    tp_size: 1
    compile_cache_size_limit: 64

model:
    gen:
        text_cfg_ratio: 0.1
        is_train: true
        scheduler:
            num_train_timesteps: 1000
            base_image_seq_len: 256
            base_shift: 0.5
            max_image_seq_len: 4096
            max_shift: 1.15
            shift: 1.0 # need consider 3.0 or 1.0
            weighting_scheme: 'logit_normal'
            logit_mean: 0.0
            logit_std: 1.0
            mode_scale: 1.29
            use_dynamic_shifting: true
        gen_transformer:
            dim: 2048
            ffn_dim_multiplier: 1.5
            multiple_of: 256
            n_heads: 32
            n_kv_heads: 8
            n_layers: 16
            ada_dim: 2048
            patch_size: 2
            in_channels: 16
            out_channels: 16
            tmb_size: 256
            gen_seqlen: 256
            condition_seqlen: 320
            norm_eps: 1e-5
            pre_trained_path:  /mnt/pollux/checkpoints/Llama-3.2-1B/original/consolidated.00.pth
            attn_type: 'full'
            plan_transformer_dim: 3072
            token_drop_ratio: 0.25
            token_mixer_depth: 8
    plan:
        text_cfg_ratio: 0.1
        image_cfg_ratio: 0.1
        latent_projector:
            latent_dim: 16
            patchify_size: 2
            output_dim: 3072
        tokenizer:
            model_name: "/mnt/pollux/checkpoints/Llama-3.2-3B"
        llm:
            dim: 3072
            ffn_dim_multiplier: 1.0
            multiple_of: 256
            n_layers: 28
            n_heads: 24
            n_kv_heads: 8
            norm_eps: 1e-5
            rope_theta: 500000.0
            pre_trained_path: /mnt/pollux/checkpoints/Llama-3.2-3B/original/consolidated.00.pth
            from_llama: true
            gen_seqlen: 256
            condition_seqlen: 320
            text_seqlen: 128

        codebook_size: 16
        is_train: false
    plan_weight: 1.0
    gen_weight: 1.0
    with_vae: true
    vae_args:
        model_name: Hunyuan
        pretrained_model_name_or_path: '/mnt/pollux/checkpoints/HunyuanVideo/vae'
        enable_tiling: false
        enable_slicing: false
data:
  - stage: stage-1
    id: 1
    data_name: bucket-hq
    task: latent_text_to_image
    source: mongodb
    partition_key: 'partition_key'
    retries: 3
    extract_field:
        "media_path": "image"
    use: True
    dataloader:
        batch_size: 1
        num_workers: 4
        seed: 1024
        shuffle: True
        pin_memory: False
        drop_last: False
    query:
        width:
            $gte: 3840
    image_size: 3840
    condition_image_size: 256

profiling:
    run: false

checkpoint:
    dump:
        every: 5000
        keep: 0 # Don't remove the ckpt
    eval:
        every: 5000
        keep: 0 # Don't remove the ckpt

logging:
    freq: 100
    wandb:
        project: Pollux
        entity: metauto
        name: ''

env:
    ENABLE_INTRA_NODE_COMM: '0'  # '0' for local machine (otherwise errors happen); '1' for slurmn (need test)
