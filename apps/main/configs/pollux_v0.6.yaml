# TEMPLATE:
# torchrun --standalone --nnodes 1 --nproc-per-node 4 -m apps.main.train config=apps/main/configs/pollux_v0.6.yaml

env:
    ENABLE_INTRA_NODE_COMM: '0' # disable one default seeting here, as have some error in our cluster

# SETUP THE DISTRIBUTION FIRST (*: LEAVE FOR FUTURE MULTI-NODES SETTING)
distributed:
    gpus: 4,5,6,7
        # - [2, 3, 4, 7]
        # - [0, 1, 2] # *
        # - [0, 1, 2, 3, 4]  # * 
    fsdp_type: full_shard 
    dp_shard: 4 # directly calculated by `gpus` in the future code
    dp_replicate: 1
    compile: false
    model_dtype: bf16 # options: `fb8` is only supported by H100
    matmul_allow_tf32: false
    selective_activation_checkpointing: false
    tp_size: 1
    compile_cache_size_limit: 64
    # nproc_per_node: -1 # directly calculated by `gpus` in the future code
    # nnodes: -1 # directly calculated by `gpus` in the future code
    # master_addr: "" # *
    # master_port: 6667 # *
    # node_rank: 0  # *

# SETUP THE SAVED PATH / WANDB LOG
name: "ImageNet_Pollux_v0.6_mctest"
output_dir: "/jfs/checkpoints/dump"
dump_dir: "" # No need now
steps: 1000000
seed: 777
optim:
    lr: 1.4e-4
    warmup: 2000
    lr_min_ratio: 0.000001
    clip: 10.0
    weight_decay: 0.01


# SETUP THE MODEL HYPERPARAMETERS
model:
    text_cfg_ratio: 0.1
    image_cfg_ratio: 0.5
    mask_patch: 16
    vae:
        pretrained_model_name_or_path: '/jfs/checkpoints/Flux-dev'
    scheduler:
        num_train_timesteps: 1000
        base_image_seq_len: 256
        base_shift: 0.5
        max_image_seq_len: 4096
        max_shift: 1.15
        shift: 3.0 # need consider 3.0 or 1.0
        weighting_scheme: 'logit_normal'
        logit_mean: 0.0
        logit_std: 1.0
        mode_scale: 1.29
        use_dynamic_shifting: true
    gen_transformer:
        dim: 2048
        ffn_dim_multiplier: 1.5
        multiple_of: 256
        n_heads: 32
        n_kv_heads: 8
        n_layers: 16
        ada_dim: 2048
        patch_size: 2
        in_channels: 16
        out_channels: 16
        tmb_size: 256
        gen_seqlen: 256
        condition_seqlen: 320
        norm_eps: 1e-5
        pre_trained_path: /jfs/checkpoints/Llama-3.2-1B/original/consolidated.00.pth
        attn_type: "bi_causal"
    plan_transformer:
        dim: 2048
        ffn_dim_multiplier: 1.5
        multiple_of: 256
        n_heads: 32
        n_kv_heads: 8
        n_layers: 16
        patch_size: 2
        in_channels: 32  
        vocab_size: 128256  
        text_seqlen: 32 
        gen_seqlen: 256 
        pre_trained_path: /jfs/checkpoints/Llama-3.2-1B/original/consolidated.00.pth
        attn_type: "bi_causal"
        norm_eps: 1e-5
    tokenizer:
        model_path: /jfs/checkpoints/Llama-3.2-1B/original/tokenizer.model

# SETUP THE DATASET
data:
    batch_size: 12
    image_size: 256
    num_workers: 8
    split: 'train'
    data_name: 'ILSVRC/imagenet-1k' # 'dummy' 
    root_dir: '/jfs/data'

profiling:
    run: true

checkpoint:
    dump:
        every: 2500
        keep: 0 # Don't remove the ckpt
    eval:
        every: 5000
        keep: 0 # Don't remove the ckpt

logging:
    freq: 100
    wandb:
        project: "Pollux"
        entity: "metauto"
        name: ""
