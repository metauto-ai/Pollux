# find /jfs/data/imagenet-1k/ -type f -name "*.lock" -exec rm -f {} \;
# rm -rf /jfs/checkpoints/dump/Pollux_v0.8.2_Local_ImageNet-1k_multimodal/ 
# torchrun --standalone --nnodes 1 --nproc-per-node 4 -m apps.plan.train config=apps/plan/configs/train.yaml
# nohup torchrun --standalone --nnodes 1 --nproc-per-node 4 -m apps.plan.train config=apps/plan/configs/train.yaml > output2.log 2>&1 &



# Set up single experiment
version: v0.8.3
# From now on, start to align train, data, and model setting with train stage (just finish refactor for dara)
train_stage: preliminary # options: preliminary, pretraining, posttraining; aligned with data setting

name: Pollux_v0.8.3_Local_ImageNet-1k_multimodal_1.4e-4_random_drop_e2e_causal

output_dir: /jfs/checkpoints/dump
dump_dir: '' # No need now
steps: 1000000
seed: 777
optim:
    lr: 1.4e-4
    warmup: 2000
    lr_min_ratio: 0.000001
    clip: 10.0
    weight_decay: 0.01
    
distributed:

    gpus: 4,5,6,7 #6,7 #,2,3 #0,1,2,3 #4,5,6,7
    fsdp_type: full_shard
    dp_shard: 4
    dp_replicate: 1
    compile: false
    model_dtype: bf16 # options: `fb8` is only supported by H100
    matmul_allow_tf32: false
    selective_activation_checkpointing: false
    tp_size: 1
    compile_cache_size_limit: 64

model:
    text_cfg_ratio: 0.1
    image_cfg_ratio: 0.1
    num_classes: 1000
    vae:
        model_name: "COSMOS-DV"
        pretrained_model_name_or_path: '/jfs/checkpoints/cosmos/Cosmos-Tokenizer-DV8x16x16'
        model_dtype: bfloat16 # should be consistent with distributed.model_dtype
        # model_name: "Hunyuan"
        # pretrained_model_name_or_path: '/jfs/checkpoints/models--tencent--HunyuanVideo/snapshots/2a15b5574ee77888e51ae6f593b2ceed8ce813e5/vae'
        enable_tiling: false
        enable_slicing: false
    latent_projector:
        latent_dim: 6
        patchify_size: 1
        output_dim: 3072
    use_vision_boi: true
    tokenizer:
        model_name: "meta-llama/Llama-3.2-3B"
    llm:
        dim: 3072
        ffn_dim_multiplier: 1.0
        multiple_of: 256
        n_layers: 28
        n_heads: 24
        n_kv_heads: 8
        norm_eps: 1e-5
        rope_theta: 500000.0
        pre_trained_path: /jfs/checkpoints/Llama-3.2-3B/original/consolidated.00.pth
        gen_seqlen: 256
        condition_seqlen: 320
        attention_type: 'causal'
    codebook_size: 64000




data:
  - stage: preliminary
    id: 0
    task: class_to_image
    data_name: ILSVRC/imagenet-1k
    source: huggingface
    batch_size: 16 # 96
    image_size: 256
    num_workers: 8
    split: train
    root_dir: /jfs/data
    cache_dir: /jfs/data/imagenet-1k
    use: True

  - stage: preliminary
    id: 1
    task: class_to_image
    data_name: imagenet-1k
    source: mongodb
    batch_size: 48 #96
    image_size: 256
    num_workers: 8
    split: train
    partition_key: 'key'
    use: False


  - stage: preliminary
    id: 2
    task: class_to_image
    data_name: dummy
    source: local
    batch_size: 12
    image_size: 256
    num_workers: 8
    split: train
    root_dir: /jfs/data
    cache_dir: ''
    use: False

  - stage: pretraining
    id: 0
    task: image_generation
    data_name: NucluesIMG-100M
    source: mongodb
    batch_size: 12
    image_size: 256
    num_workers: 8
    mongo_uri: mongodb://localhost:27017
    use: False

profiling:
    run: false

checkpoint:
    dump:
        every: 5000
        keep: 0 # Don't remove the ckpt
    eval:
        every: 5000
        keep: 0 # Don't remove the ckpt
logging:
    freq: 200
    wandb:
        project: Pollux
        entity: metauto
        name: ''


env:
    ENABLE_INTRA_NODE_COMM: '0'  # '0' for local machine (otherwise errors happen); '1' for slurmn (need test)