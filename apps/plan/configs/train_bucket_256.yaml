# torchrun --standalone --nnodes 1 --nproc-per-node 4 -m apps.plan.train config=apps/plan/configs/train_bucket_256.yaml
# nohup torchrun --standalone --nnodes 1 --nproc-per-node 4 -m apps.plan.train config=apps/plan/configs/train.yaml > output2.log 2>&1 &



# Set up single experiment
version: v1.0
# From now on, start to align train, data, and model setting with train stage (just finish refactor for dara)
train_stage: stage-1 # options: preliminary, pretraining, posttraining; aligned with data setting

name: Pollux_bucket-256-1_hz_test

output_dir: /jfs/checkpoints/dump
dump_dir: '' # No need now
steps: 1000000
seed: 777
optim:
    lr: 1.4e-4
    weight_decay: 0.01
    warmup: 2000
    lr_min_ratio: 0.000001
    clip: 10.0
    
distributed:

    gpus: 0,1 #6,7 #,2,3 #0,1,2,3 #4,5,6,7
    fsdp_type: full_shard
    dp_shard: 4
    dp_replicate: 1
    compile: false
    model_dtype: bf16 # options: `fb8` is only supported by H100
    matmul_allow_tf32: false
    selective_activation_checkpointing: false
    tp_size: 1
    compile_cache_size_limit: 64

model:
    text_cfg_ratio: 0.1
    latent_projector:
        latent_dim: 6
        patchify_size: 1
        output_dim: 3072
    tokenizer:
        model_name: "/jfs/checkpoints/Llama-3.2-3B"
    llm:
        dim: 3072
        ffn_dim_multiplier: 1.0
        multiple_of: 256
        n_layers: 28
        n_heads: 24
        n_kv_heads: 8
        norm_eps: 1e-5
        rope_theta: 500000.0
        pre_trained_path: /jfs/checkpoints/Llama-3.2-3B/original/consolidated.00.pth
        from_llama: true
        gen_seqlen: 256
        condition_seqlen: 320
        text_seqlen: 128
    codebook_size: 64000


data:  
  - stage: stage-1
    id: 1
    data_name: bucket-256-parquet
    task: latent_text_to_image
    source: mongodb
    partition_key: 'partition_key'
    retries: 3
    extract_field:
        "parquet_size": "sample_num"
        "parquet_path": "path"
    mapping_field:
        "Cosmos_latent_code": "latent_code"
        "Cosmos_latent_code_indices": "latent_code_indices"
        "caption": "caption"
    use: True
    dataloader:
        batch_size: 24
        num_workers: 4
        seed: 1024
        shuffle: False
        pin_memory: False
        drop_last: False

profiling:
    run: false

checkpoint:
    dump:
        every: 5000
        keep: 0 # Don't remove the ckpt
    eval:
        every: 5000
        keep: 0 # Don't remove the ckpt
logging:
    freq: 100
    wandb:
        project: Pollux
        entity: metauto
        name: ''


env:
    ENABLE_INTRA_NODE_COMM: '0'  # '0' for local machine (otherwise errors happen); '1' for slurmn (need test)