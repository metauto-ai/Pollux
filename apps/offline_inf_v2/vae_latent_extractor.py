from collections import defaultdict
import json
import os
from tqdm import tqdm
from PIL import Image
import cupy as cp
from crossfit.backend.cudf.series import create_list_series_from_1d_or_2d_ar

import nvidia.dali.fn as fn
import nvidia.dali.types as types
import torch
from nvidia.dali import pipeline_def
from nvidia.dali.plugin.pytorch import feed_ndarray
from nvidia.dali.tensors import TensorListGPU

from nemo_curator.datasets import ImageTextPairDataset
from nemo_curator.utils.distributed_utils import load_object_on_worker

from apps.main.modules.preprocess import generate_crop_size_list, var_center_crop_size_fn
from apps.offline_inf_v2.data import DataArgs
from apps.offline_inf_v2.model import ModelArgs, VAE


@pipeline_def
def image_loading_pipeline(_tar_path: str, use_index_files: bool, data_args: DataArgs):
    if use_index_files:
        index_paths = [f"{_tar_path.rsplit('.', 1)[0]}.idx"]
    else:
        index_paths = []

    images_raw, text, json = fn.readers.webdataset(
        paths=_tar_path,
        index_paths=index_paths,
        ext=["jpg", "txt", "json"],
        missing_component_behavior="error",
    )
    images = fn.decoders.image(images_raw, device="mixed", output_type=types.RGB)
    return images, text, json


@pipeline_def
def webdataset_pipeline(_tar_path: str, use_index_files: bool, data_args: DataArgs):
    if use_index_files:
        index_paths = [f"{_tar_path.rsplit('.', 1)[0]}.idx"]
    else:
        index_paths = []

    images_raw, text, json = fn.readers.webdataset(
        paths=_tar_path,
        index_paths=index_paths,
        ext=["jpg", "txt", "json"],
        missing_component_behavior="error",
    )
    images = fn.decoders.image(images_raw, device="mixed", output_type=types.RGB)
    
    images = fn.resize(
        images, 
        device="gpu",
        resize_x=data_args.image_size, 
        resize_y=data_args.image_size,
        mode="not_smaller", 
        interp_type=types.DALIInterpType.INTERP_CUBIC
    )

    # get the dynamic crop size
    crop_size = fn.python_function(
        images.shape(device="cpu"),
        data_args.image_size,
        data_args.patch_size,
        data_args.dynamic_crop_ratio,
        function=var_center_crop_size_fn,
        device="cpu"
    )

    images = fn.crop_mirror_normalize(
        images, 
        device="gpu",
        crop_h=crop_size[0], 
        crop_w=crop_size[1],
        crop_pos_x=0.5, 
        crop_pos_y=0.5, 
        mirror=fn.random.coin_flip(probability=0.5),
        dtype=types.DALIDataType.FLOAT, 
        mean=[0.5 * 255, 0.5 * 255, 0.5 * 255], 
        std=[0.5 * 255, 0.5 * 255, 0.5 * 255],
        scale=1.0,
    )

    return images, text, json


class VAELatentExtractor:
    def __init__(
        self,
        model_args: ModelArgs,
        data_args: DataArgs,
        use_index_files: bool = False,
    ) -> None:
        """
        Constructs the embedder.

        Args:
            model_args (ModelArgs): The arguments for the VAE.
            data_args (DataArgs): The arguments for the data.
            use_index_files (bool): If True, tries to find and use index files generated
                by DALI at the same path as the tar file shards. The index files must be
                generated by DALI's wds2idx tool. See https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/dataloading_webdataset.html#Creating-an-index
                for more information. Each index file must be of the form "shard_id.idx"
                where shard_id is the same integer as the corresponding tar file for the
                data. The index files must be in the same folder as the tar files.
        """
        self.model_args = model_args
        self.data_args = data_args
        self.use_index_files = use_index_files

    def load_dataset_shard(self, tar_path: str):
        """
        Loads a WebDataset tar shard using DALI.

        Args:
            tar_path (str): The path of the tar shard to load.

        Returns:
            Iterable: An iterator over the dataset. Each tar file
            must have 3 files per record: a .jpg file, a .txt file,
            and a .json file. The .jpg file must contain the image, the
            .txt file must contain the associated caption, and the
            .json must contain the metadata for the record (including
            its ID). Images will be loaded using DALI.
        """
        # # Create the DALI pipeline
        # @pipeline_def(
        #     batch_size=self.data_args.batch_size,
        #     num_threads=self.data_args.num_threads_per_worker,
        #     device_id=0,
        #     exec_dynamic=True,
        # )
        # def webdataset_pipeline_wrapper(_tar_path: str, use_index_files: bool, data_args: DataArgs):
        #     return webdataset_pipeline(_tar_path, use_index_files, data_args)

        dali_pipeline_args = {
            "batch_size": self.data_args.batch_size,
            "num_threads": self.data_args.num_threads_per_worker,
            "device_id": 0,
            "exec_dynamic": True,
        }

        if self.data_args.enable_checkpointing:
            dali_pipeline_args["enable_checkpointing"] = True
            checkpoint_path = f"{tar_path.rsplit('.', 1)[0]}_{self.data_args.image_size}.pth"
            if os.path.exists(checkpoint_path):
                print (f"Restoring checkpoint from {checkpoint_path}")
                checkpoint = open(checkpoint_path, 'rb').read()
                dali_pipeline_args["checkpoint"] = checkpoint

        pipeline = webdataset_pipeline(
            tar_path, 
            self.use_index_files, 
            self.data_args,
            **dali_pipeline_args,
        )
        pipeline.build()

        total_samples = pipeline.epoch_size()
        total_samples = total_samples[list(total_samples.keys())[0]]

        crop_size_list = generate_crop_size_list(
            image_size=self.data_args.image_size, 
            patch_size=self.data_args.patch_size, 
            max_ratio=2.0
        )

        bucket_img = defaultdict(list)
        # bucket_text_cpu = defaultdict(list)
        bucket_meta = defaultdict(list)

        # pbar = tqdm(total=total_samples, desc="Loading dataset shard")

        samples_completed = 0
        while samples_completed < total_samples:
            image, text, meta = pipeline.run()
            for i in range(self.data_args.batch_size):
                img = image[i]
                _, w, h = img.shape()
                crop_size = (w, h)
                bucket_img[crop_size].append(img)
                # bucket_text_cpu[crop_size].append(text.at(i).tostring().decode("utf-8"))
                bucket_meta[crop_size].append(json.loads(meta.at(i).tostring().decode("utf-8")))

                # if batch size is reached, yield the batch
                if len(bucket_img[crop_size]) == self.data_args.batch_size:
                    image_batch = TensorListGPU(bucket_img[crop_size]).as_tensor()
                    image_torch = torch.empty(image_batch.shape(), dtype=torch.float32, device="cuda")
                    feed_ndarray(image_batch, image_torch)  # COPY !!!
                    yield image_torch, bucket_meta[crop_size]
                    bucket_img[crop_size] = []
                    bucket_meta[crop_size] = []

            samples_completed += self.data_args.batch_size

            # if samples_completed % (100 * self.data_args.batch_size) == 0 and self.data_args.enable_checkpointing is not None:
            #     pipeline.checkpoint(checkpoint_path)
            # pbar.update(self.data_args.batch_size)
        # pbar.close()

        for crop_size in crop_size_list:
            if not bucket_img[crop_size]:
                continue
            image_batch = TensorListGPU(bucket_img[crop_size]).as_tensor()
            image_torch = torch.empty(image_batch.shape(), dtype=torch.float32, device="cuda")
            feed_ndarray(image_batch, image_torch)  # COPY !!!
            yield image_torch, bucket_meta[crop_size]
        
        # checkpoint final state
        if self.data_args.enable_checkpointing:
            pipeline.checkpoint(checkpoint_path)

    def load_model(self, model_args, device="cuda"):
        """
        Loads the model used to generate image embeddings.

        Args:
            device (str): A PyTorch device identifier that specifies what GPU
                to load the model on.

        Returns:
            Callable: A model loaded on the specified device.
                The model's forward call may be augmented with torch.autocast()
        """
        model = VAE(model_args).eval().to(device)
        model = self._configure_forward(model)
        return model

    def _configure_forward(self, model):
        original_forward = model.forward

        def custom_forward(*args, **kwargs):
            if self.model_args.autocast:
                with torch.amp.autocast(device_type="cuda"):
                    image_features = original_forward(*args, **kwargs)
            else:
                image_features = original_forward(*args, **kwargs)

            # Inference can be done in lower precision, but cuDF can only handle fp32
            return image_features.to(torch.float32)

        model.forward = custom_forward
        return model
        
    def _process_batch(self, model, batch):
        """Helper method to process a batch with appropriate chunking"""
        if batch.shape[0] > 16 and batch.shape[0] % 16 == 0:
            # Process in chunks of 16 to avoid OOM
            sub_batches = batch.chunk(batch.shape[0] // 16)
            sub_latents = []
            for sub_batch in sub_batches:
                sub_latent = model(sub_batch)
                sub_latents.append(sub_latent.cpu())
            return torch.cat(sub_latents, dim=0)
        else:
            latents = model(batch)
            return latents.cpu()  # Move to CPU immediately

    def __call__(self, dataset: ImageTextPairDataset) -> ImageTextPairDataset:
        """
        Generates image embeddings for all images in the dataset.

        Args:
            dataset (ImageTextPairDataset): The dataset to create image embeddings for.

        Returns:
            ImageTextPairDataset: A dataset with image embeddings and potentially
                classifier scores.
        """
        meta = dataset.metadata.dtypes.to_dict()
        meta[self.data_args.image_latent_column] = "object"
        meta[self.data_args.image_latent_shape_column] = "object"

        embedding_df = dataset.metadata.map_partitions(
            self._run_inference, dataset.tar_files, dataset.id_col, meta=meta
        )

        return ImageTextPairDataset(
            dataset.path,
            metadata=embedding_df,
            tar_files=dataset.tar_files,
            id_col=dataset.id_col,
        )

    def _run_inference(self, partition, tar_paths, id_col, partition_info=None):
        tar_path = tar_paths[partition_info["number"]]
        device = "cuda"

        model = load_object_on_worker(
            "model",
            self.load_model,
            {"model_args": self.model_args, "device": device},
        )

        print(f"Model loaded on {device}")

        dataset = self.load_dataset_shard(tar_path)
        final_image_latents = []
        image_ids = []
        samples_completed = 0
        expected_samples = len(partition)
        progress_bar = tqdm(
            total=expected_samples,
            desc=f"{tar_path} - Latent extraction with {self.model_args.gen_vae.model_name}",
        )
        
        # Process batches
        with torch.no_grad(), torch.amp.autocast(device_type="cuda", enabled=self.model_args.autocast):
            for batch, metadata in dataset:
                # Only process as many samples as we expect from metadata
                if samples_completed >= expected_samples:
                    break
                
                # Calculate how many samples we should process from this batch
                remaining = expected_samples - samples_completed
                actual_batch_size = min(len(metadata), remaining)
                
                if actual_batch_size < len(metadata):
                    # Truncate batch and metadata if needed
                    batch = batch[:actual_batch_size]
                    metadata = metadata[:actual_batch_size]
                
                image_latents = self._process_batch(model, batch)
                del batch

                final_image_latents.append(image_latents)
                image_ids.extend(m[id_col] for m in metadata)

                samples_completed += actual_batch_size
                progress_bar.update(actual_batch_size)
                
                # Clear CUDA cache frequently
                if samples_completed % (self.data_args.batch_size * 5) == 0:
                    torch.cuda.empty_cache()
        progress_bar.close()

        if samples_completed != len(partition):
            raise RuntimeError(
                f"Mismatch in sample count for partition {partition_info['number']}. "
                f"{len(partition)} samples found in the metadata, but {samples_completed} found in {tar_path}."
            )

        # Process embeddings in memory-efficient way
        return self._process_embeddings(partition, final_image_latents, image_ids)
        
    def _process_embeddings(self, partition, final_image_latents, image_ids):
        """Process embeddings in a memory-efficient way"""
        
        # Check if we need to handle variable-sized latents
        if self.data_args.dynamic_crop_ratio > 1.0:
            # Handle variable-sized latents
            return self._process_variable_sized_embeddings(partition, final_image_latents, image_ids)
        else:
            # Order the output of the shard
            sorted_indices = sorted(range(len(image_ids)), key=lambda k: image_ids[k])
            # Process fixed-size latents as before
            # Process in chunks to reduce memory usage
            all_embeddings = torch.cat(final_image_latents, dim=0)
            sorted_embeddings = all_embeddings[sorted_indices]
            
            embedding_shape_list = [emb.shape for emb in sorted_embeddings]

            # View the embeddings to be [N, 16*32*32]
            sorted_embeddings = sorted_embeddings.view(sorted_embeddings.shape[0], -1)
            
            # Process in chunks to avoid OOM
            chunk_size = 1000  # Adjust based on your GPU memory
            concat_embedding_output = None
            for i in range(0, sorted_embeddings.shape[0], chunk_size):
                end_idx = min(i + chunk_size, sorted_embeddings.shape[0])
                chunk = sorted_embeddings[i:end_idx].cuda()  # Move chunk to GPU
                chunk_cp = cp.asarray(chunk)  # Convert to CuPy
                
                if concat_embedding_output is None:
                    concat_embedding_output = chunk_cp
                else:
                    concat_embedding_output = cp.concatenate([concat_embedding_output, chunk_cp], axis=0)
                
                # Free GPU memory
                del chunk
                torch.cuda.empty_cache()

            partition[self.data_args.image_latent_column] = create_list_series_from_1d_or_2d_ar(
                concat_embedding_output, index=partition.index
            )
            
            # Convert embedding_shape_list to a CuPy array before passing it
            embedding_shape_array = cp.array(embedding_shape_list)
            partition[self.data_args.image_latent_shape_column] = create_list_series_from_1d_or_2d_ar(
                embedding_shape_array, index=partition.index
            )

            del concat_embedding_output
            del final_image_latents
            del embedding_shape_array  # Also clean up the new array
            torch.cuda.empty_cache()

            return partition
            
    def _process_variable_sized_embeddings(self, partition, final_image_latents, image_ids):
        """Process embeddings with variable sizes due to dynamic cropping"""
        # Order the output of the shard
        sorted_indices = sorted(range(len(image_ids)), key=lambda k: image_ids[k])

        # Flatten our list of tensors
        flat_embeddings = []
        current_idx = 0
        
        for batch_tensors in final_image_latents:
            batch_size = batch_tensors.shape[0]
            for i in range(batch_size):
                flat_embeddings.append(batch_tensors[i])
                current_idx += 1
        
        # Create sorted embeddings list
        sorted_embeddings = [flat_embeddings[idx] for idx in sorted_indices]
        del flat_embeddings
        
        # Process each embedding individually and create a list of numpy arrays
        embedding_list = []
        embedding_shape_list = []
        for emb in sorted_embeddings:
            # Flatten the embedding to 1D
            embedding_shape_list.append(emb.shape)
            flat_emb = emb.view(-1).numpy()  # Convert to numpy array
            embedding_list.append(flat_emb)
            
        # Assign to partition
        partition[self.data_args.image_latent_column] = embedding_list
        partition[self.data_args.image_latent_shape_column] = embedding_shape_list
        
        del embedding_list
        del sorted_embeddings
        del final_image_latents
        torch.cuda.empty_cache()
        
        return partition