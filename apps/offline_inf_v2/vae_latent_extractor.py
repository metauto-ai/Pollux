import json
from tqdm import tqdm
from PIL import Image
import cupy as cp
from crossfit.backend.cudf.series import create_list_series_from_1d_or_2d_ar

import nvidia.dali.fn as fn
import nvidia.dali.types as types
import torch
from nvidia.dali import pipeline_def
from nvidia.dali.plugin.pytorch import feed_ndarray

from nemo_curator.datasets import ImageTextPairDataset
from nemo_curator.utils.distributed_utils import load_object_on_worker

from apps.offline_inf_v2.data import DataArgs
from apps.offline_inf_v2.model import ModelArgs, VAE


class VAELatentExtractor:
    def __init__(
        self,
        model_args: ModelArgs,
        data_args: DataArgs,
        use_index_files: bool = False,
    ) -> None:
        """
        Constructs the embedder.

        Args:
            model_args (ModelArgs): The arguments for the VAE.
            data_args (DataArgs): The arguments for the data.
            use_index_files (bool): If True, tries to find and use index files generated
                by DALI at the same path as the tar file shards. The index files must be
                generated by DALI's wds2idx tool. See https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/data_loading/dataloading_webdataset.html#Creating-an-index
                for more information. Each index file must be of the form "shard_id.idx"
                where shard_id is the same integer as the corresponding tar file for the
                data. The index files must be in the same folder as the tar files.
        """
        self.model_args = model_args
        self.data_args = data_args
        self.use_index_files = use_index_files

        # torch_transforms = transforms.Compose(
        #     [
        #         transforms.RandomHorizontalFlip(),
        #         transforms.ToTensor(),
        #         transforms.Normalize(
        #             mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True
        #         ),
        #     ]
        # )
        # self.dali_transforms = convert_transforms_to_dali(torch_transforms)

    def load_dataset_shard(self, tar_path: str):
        """
        Loads a WebDataset tar shard using DALI.

        Args:
            tar_path (str): The path of the tar shard to load.

        Returns:
            Iterable: An iterator over the dataset. Each tar file
            must have 3 files per record: a .jpg file, a .txt file,
            and a .json file. The .jpg file must contain the image, the
            .txt file must contain the associated caption, and the
            .json must contain the metadata for the record (including
            its ID). Images will be loaded using DALI.
        """

        
        def downsample_resize_image(image):
            """
            Center cropping implementation from ADM.
            https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126
            """
            image = cp.asnumpy(image)
            pil_image = Image.fromarray(image)
            while min(*pil_image.size) >= 2 * self.data_args.image_size:
                pil_image = pil_image.resize(
                    tuple(x // 2 for x in pil_image.size), resample=Image.BOX
                )
            return cp.asarray(pil_image)
            

        # Create the DALI pipeline
        @pipeline_def(
            batch_size=self.data_args.batch_size,
            num_threads=self.data_args.num_threads_per_worker,
            device_id=0,
        )
        def webdataset_pipeline(_tar_path: str):
            if self.use_index_files:
                index_paths = [f"{_tar_path.rsplit('.', 1)[0]}.idx"]
            else:
                index_paths = []

            images_raw, text, json = fn.readers.webdataset(
                paths=_tar_path,
                index_paths=index_paths,
                ext=["jpg", "txt", "json"],
                missing_component_behavior="error",
            )
            images = fn.decoders.image(images_raw, device="mixed", output_type=types.RGB)
            
            # images = fn.python_function(
            #     images,
            #     function=downsample_resize_image,
            # )
            images = fn.resize(
                images, 
                device="gpu",
                resize_x=self.data_args.image_size, 
                resize_y=self.data_args.image_size,
                mode="not_smaller", 
                interp_type=types.DALIInterpType.INTERP_CUBIC
            )
            images = fn.crop_mirror_normalize(
                images, 
                device="gpu",
                crop_h=self.data_args.image_size, 
                crop_w=self.data_args.image_size,
                crop_pos_x=0.5, 
                crop_pos_y=0.5, 
                mirror=fn.random.coin_flip(probability=0.5),
                dtype=types.DALIDataType.FLOAT, 
                mean=[0.5 * 255, 0.5 * 255, 0.5 * 255], 
                std=[0.5 * 255, 0.5 * 255, 0.5 * 255],
                scale=1.0,
            )

            return images, text, json

        pipeline = webdataset_pipeline(tar_path)
        pipeline.build()

        total_samples = pipeline.epoch_size()
        total_samples = total_samples[list(total_samples.keys())[0]]

        samples_completed = 0
        while samples_completed < total_samples:
            image, text, meta = pipeline.run()
            image = image.as_tensor()

            image_torch = torch.empty(image.shape(), dtype=torch.float32, device="cuda")
            feed_ndarray(image, image_torch)  # COPY !!!
            image = image_torch

            captions = [text.at(i).tostring().decode("utf-8") for i in range(len(text))]
            metadata = [
                json.loads(meta.at(i).tostring().decode("utf-8"))
                for i in range(len(meta))
            ]

            remaining_samples = total_samples - samples_completed
            if image.shape[0] >= remaining_samples:
                image = image[:remaining_samples]
                captions = captions[:remaining_samples]
                metadata = metadata[:remaining_samples]

            samples_completed += min(image.shape[0], remaining_samples)

            yield image, metadata

    def load_model(self, model_args, device="cuda"):
        """
        Loads the model used to generate image embeddings.

        Args:
            device (str): A PyTorch device identifier that specifies what GPU
                to load the model on.

        Returns:
            Callable: A timm model loaded on the specified device.
                The model's forward call may be augmented with torch.autocast()
                or embedding normalization if specified in the constructor.
        """
        model = VAE(model_args).eval().to(device)
        model = self._configure_forward(model)
        
        return model

    def _configure_forward(self, model):
        original_forward = model.forward

        def custom_forward(*args, **kwargs):
            if self.model_args.autocast:
                with torch.amp.autocast(device_type="cuda"):
                    image_features = original_forward(*args, **kwargs)
            else:
                image_features = original_forward(*args, **kwargs)

            # Inference can be done in lower precision, but cuDF can only handle fp32
            return image_features.to(torch.float32)

        model.forward = custom_forward
        return model
        
    def _process_batch(self, model, batch):
        """Helper method to process a batch with appropriate chunking"""
        if batch.shape[0] > 16 and batch.shape[0] % 16 == 0:
            # Process in chunks of 16 to avoid OOM
            sub_batches = batch.chunk(batch.shape[0] // 16)
            sub_latents = []
            for sub_batch in sub_batches:
                sub_latent = model(sub_batch)
                sub_latents.append(sub_latent.cpu())
            return torch.cat(sub_latents, dim=0)
        else:
            latents = model(batch)
            return latents.cpu()  # Move to CPU immediately

    def __call__(self, dataset: ImageTextPairDataset) -> ImageTextPairDataset:
        """
        Generates image embeddings for all images in the dataset.

        Args:
            dataset (ImageTextPairDataset): The dataset to create image embeddings for.

        Returns:
            ImageTextPairDataset: A dataset with image embeddings and potentially
                classifier scores.
        """
        meta = dataset.metadata.dtypes.to_dict()
        meta[self.data_args.image_latent_column] = "object"

        embedding_df = dataset.metadata.map_partitions(
            self._run_inference, dataset.tar_files, dataset.id_col, meta=meta
        )

        return ImageTextPairDataset(
            dataset.path,
            metadata=embedding_df,
            tar_files=dataset.tar_files,
            id_col=dataset.id_col,
        )

    def _run_inference(self, partition, tar_paths, id_col, partition_info=None):
        tar_path = tar_paths[partition_info["number"]]
        device = "cuda"

        model = load_object_on_worker(
            "model",
            self.load_model,
            {"model_args": self.model_args, "device": device},
        )

        dataset = self.load_dataset_shard(tar_path)
        final_image_latents = []
        image_ids = []
        samples_completed = 0
        progress_bar = tqdm(
            total=len(partition),
            desc=f"{tar_path} - Latent extraction with {self.model_args.gen_vae.model_name}",
        )
        
        # Process batches
        with torch.no_grad(), torch.amp.autocast(device_type="cuda", enabled=self.model_args.autocast):
            for batch, metadata in dataset:
                image_latents = self._process_batch(model, batch)
                
                final_image_latents.append(image_latents)
                image_ids.extend(m[id_col] for m in metadata)

                batch_size = len(image_latents)
                samples_completed += batch_size
                progress_bar.update(batch_size)
                
                # Clear CUDA cache less frequently
                if samples_completed % (self.data_args.batch_size * 50) == 0:
                    torch.cuda.empty_cache()
        progress_bar.close()

        if samples_completed != len(partition):
            raise RuntimeError(
                f"Mismatch in sample count for partition {partition_info['number']}. "
                f"{len(partition)} samples found in the metadata, but {samples_completed} found in {tar_path}."
            )

        # Process embeddings in memory-efficient way
        return self._process_embeddings(partition, final_image_latents, image_ids)
        
    def _process_embeddings(self, partition, final_image_latents, image_ids):
        """Process embeddings in a memory-efficient way"""
        # Order the output of the shard
        sorted_indices = sorted(range(len(image_ids)), key=lambda k: image_ids[k])
        
        # Process in chunks to reduce memory usage
        all_embeddings = torch.cat(final_image_latents, dim=0)
        sorted_embeddings = all_embeddings[sorted_indices]
        
        # View the embeddings to be [N, 16*32*32]
        sorted_embeddings = sorted_embeddings.view(sorted_embeddings.shape[0], -1)
        
        # Process in chunks to avoid OOM
        chunk_size = 1000  # Adjust based on your GPU memory
        concat_embedding_output = None
        
        for i in range(0, sorted_embeddings.shape[0], chunk_size):
            end_idx = min(i + chunk_size, sorted_embeddings.shape[0])
            chunk = sorted_embeddings[i:end_idx].cuda()  # Move chunk to GPU
            chunk_cp = cp.asarray(chunk)  # Convert to CuPy
            
            if concat_embedding_output is None:
                concat_embedding_output = chunk_cp
            else:
                concat_embedding_output = cp.concatenate([concat_embedding_output, chunk_cp], axis=0)
            
            # Free GPU memory
            del chunk
            torch.cuda.empty_cache()
        
        partition[self.data_args.image_latent_column] = create_list_series_from_1d_or_2d_ar(
            concat_embedding_output, index=partition.index
        )

        return partition
