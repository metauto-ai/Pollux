# python -m apps.gen_tran.generate config=apps/gen_tran/configs/eval.yaml
# torchrun --nnodes 1 --nproc-per-node 4 -m apps.gen_tran.eval config=apps/gen_tran/configs/eval.yaml
name: "debug_evals"
stage: eval
ckpt_dir: "/jfs/checkpoints/dump/Pollux_v1.0_latent_pollux_full_attn_from_scratch_clip_embedding_parquet/checkpoints/0000005000"
dump_dir: "/jfs/checkpoints/dump/Pollux_v1.0_latent_pollux_full_attn_from_scratch_clip_embedding_parquet/checkpoints/0000005000"
generator:
  guidance_scale: 2.5
  dtype: bf16
  resolution: 256
  show_progress: False
  inference_steps: 100
  vae_scale_factor: 8.0
  tvae:
    model_name: Hunyuan
    pretrained_model_name_or_path: '/jfs/checkpoints/models--tencent--HunyuanVideo/snapshots/2a15b5574ee77888e51ae6f593b2ceed8ce813e5/vae'
    enable_tiling: false
    enable_slicing: false
  # text_encoder:
  #     dim: 3072
  #     ffn_dim_multiplier: 1.0
  #     multiple_of: 256
  #     n_heads: 24
  #     n_kv_heads: 8
  #     n_layers: 28 
  #     vocab_size: 128256  
  #     text_seqlen: 128 
  #     pre_trained_path: /jfs/checkpoints/Llama-3.2-3B/original/consolidated.00.pth
  #     norm_eps: 1e-5
  text_encoder:
    config_name: "ViT-B/32"
  tokenizer:
      model_path: "/jfs/checkpoints/Llama-3.2-3B/original/tokenizer.model"

eval_data:
  - stage: eval
    id: 0
    task: class_to_image
    data_name: imagenet-1k
    source: mongodb
    batch_size: 256
    image_size: 256
    num_workers: 8
    split: validation
    use: True